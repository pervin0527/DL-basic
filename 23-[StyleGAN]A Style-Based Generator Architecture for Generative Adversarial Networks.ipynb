{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = \"/home/pervinco/Datasets/CelebA\"\n",
    "save_dir = \"./runs/StyleGAN\"\n",
    "\n",
    "lr = 0.0001\n",
    "batch_sizes = [256, 256, 128, 64, 32, 16]\n",
    "\n",
    "nc = 3\n",
    "nz = 512\n",
    "nw = 512\n",
    "ndf = 512\n",
    "ngf = 512\n",
    "\n",
    "min_img_size = 4\n",
    "max_img_size = 128\n",
    "gp_coeff = 10\n",
    "num_steps = int(math.log2(max_img_size / 4)) + 1\n",
    "\n",
    "progressive_epochs = [30] * len(batch_sizes)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(img_size):\n",
    "    dataset = datasets.ImageFolder(root=data_dir,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize(img_size),\n",
    "                                        transforms.CenterCrop(img_size),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                    ]))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_sizes[int(math.log2(img_size / 4))], shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n",
    "    \n",
    "\n",
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.scale = (2 / in_features) ** 0.5\n",
    "        self.bias = self.linear.bias\n",
    "        self.linear.bias = None\n",
    "\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x * self.scale) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping Network\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, nz, nw):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(PixelNorm(),\n",
    "                                     EqualLinear(nz, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw),\n",
    "                                     nn.ReLU(),\n",
    "                                     EqualLinear(nw, nw))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.mapping(x)\n",
    "    \n",
    "\n",
    "## Adaptive Instance Normalization\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, nw):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale = EqualLinear(nw, channels)\n",
    "        self.style_bias = EqualLinear(nw, channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        \n",
    "        return style_scale * x + style_bias\n",
    "    \n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n",
    "        return x + self.weight + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGeneratorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nw):\n",
    "        super().__init__()\n",
    "        self.conv1 = EqualConv2d(in_channels, out_channels)\n",
    "        self.conv2 = EqualConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.adain1 = AdaIN(out_channels, nw)\n",
    "        self.adain2 = AdaIN(out_channels, nw)\n",
    "\n",
    "        self.noise_inject1 = NoiseInjection(out_channels)\n",
    "        self.noise_inject2 = NoiseInjection(out_channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.conv1(x)\n",
    "        x = self.noise_inject1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.adain1(x, w)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.noise_inject2(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.adain2(x, w)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = EqualConv2d(in_channels, out_channels)\n",
    "        self.conv2 = EqualConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, nw, ngf, nc):\n",
    "        super().__init__()\n",
    "        self.const_input = nn.Parameter(torch.ones(1, ngf, 4, 4))\n",
    "        self.map = MappingNetwork(nz, nw)\n",
    "\n",
    "        self.initial_adain1 = AdaIN(ngf, nw)\n",
    "        self.initial_adain2 = AdaIN(ngf, nw)\n",
    "\n",
    "        self.initial_noise1 = NoiseInjection(ngf)\n",
    "        self.initial_noise2 = NoiseInjection(ngf)\n",
    "        self.initial_conv = nn.Conv2d(ngf, ngf, kernel_size=3, stride=1, padding=1)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.initial_rgb = EqualConv2d(ngf, nc, kernel_size = 1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.toRGBs = (nn.ModuleList([]), nn.ModuleList([self.initial_rgb]))\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c  = int(ngf * factors[i])\n",
    "            conv_out_c = int(ngf * factors[i+1])\n",
    "            self.prog_blocks.append(StyleGeneratorBlock(conv_in_c, conv_out_c, nw))\n",
    "            self.toRGBs.append(EqualConv2d(conv_out_c, nc, kernel_size = 1, stride=1, padding=0))\n",
    "        \n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1-alpha ) * upscaled)\n",
    "\n",
    "\n",
    "    def forward(self, noise, alpha, steps):\n",
    "        w = self.map(noise)\n",
    "        x = self.initial_adain1(self.initial_noise1(self.const_input),w)\n",
    "        x = self.initial_conv(x)\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(x)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode = 'bilinear')\n",
    "            out = self.prog_blocks[step](upscaled,w)\n",
    "\n",
    "        final_upscaled = self.toRGBs[steps-1](upscaled)\n",
    "        final_out = self.toRGBs[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.fromRGBs = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(ndf * factors[i])\n",
    "            conv_out = int(ndf * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
    "            self.fromRGBs.append(EqualConv2d(nc, conv_in, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "\n",
    "        self.initial_rgb = EqualConv2d(nc, ndf, kernel_size=1, stride=1, padding=0)\n",
    "        self.fromRGBs.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(EqualConv2d(ndf + 1, ndf, kernel_size=3, padding=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         EqualConv2d(ndf, ndf, kernel_size=4, padding=0, stride=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         EqualConv2d(ndf, 1, kernel_size=1, padding=0, stride=1))\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3]))\n",
    "\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.fromRGBs[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.fromRGBs[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        \n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    bs, c, h, w = real.shape\n",
    "    beta = torch.rand((bs,1,1,1)).repeat(1,c, h,w).to(device)\n",
    "    \n",
    "    interpolated_images = real * beta + fake.detach() * (1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    ## Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images,alpha,train_step)\n",
    "    \n",
    "    ## Take the gradient of the scores with respect to the image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fake_images(epoch, G, fixed_noise, alpha, step, save_dir, num_images=64):\n",
    "    with torch.no_grad():  # 그래디언트 계산을 하지 않음\n",
    "        fake_images = G(fixed_noise, alpha, step).detach().cpu()\n",
    "        # 이미지의 값 범위를 [0, 1]로 조정\n",
    "        fake_images = (fake_images * 0.5) + 0.5\n",
    "\n",
    "    # 이미지 그리드 생성\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Fake Images at Epoch {epoch}\")\n",
    "    grid = vutils.make_grid(fake_images[:num_images], padding=2, normalize=False)\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "\n",
    "    # 이미지 저장\n",
    "    plt.savefig(f\"{save_dir}/Epoch_{epoch}_Fake.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(nz, nw, ngf, nc).to(device)\n",
    "D = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "g_optimizer = torch.optim.Adam([{'params': [param for name, param in G.named_parameters() if 'map' not in name]},\n",
    "                                {'params': G.map.parameters(), 'lr': 1e-5}], lr=lr, betas =(0.5, 0.99))\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.99))\n",
    "\n",
    "global_epochs = 0\n",
    "step = int(math.log2(min_img_size / 4))\n",
    "fixed_noise = torch.randn(64, nz).to(device)\n",
    "\n",
    "# 프로그레시브 학습 단계를 위한 루프\n",
    "for n_epochs in progressive_epochs[step:]:\n",
    "    alpha = 0.00001\n",
    "    dataset, dataloader = get_dataset(img_size=4*2**step)\n",
    "    print(f\"Image size:{4*2**step} | Current step:{step}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for idx, (images, _) in enumerate(tqdm(dataloader, desc=\"Train\", leave=False)):\n",
    "            bs = images.size(0)\n",
    "            real_images = images.to(device)\n",
    "            z = torch.randn(bs, nz).to(device)\n",
    "\n",
    "            # Discriminator 학습\n",
    "            fake_images = G(z, alpha, step)\n",
    "            d_real_loss = D(real_images, alpha, step).mean()\n",
    "            d_fake_loss = D(fake_images.detach(), alpha, step).mean()\n",
    "            gp = gradient_penalty(D, real_images, fake_images, alpha, step, device=device)\n",
    "            d_loss = -(d_real_loss - d_fake_loss) + gp_coeff * gp + 0.001 * (d_real_loss ** 2)\n",
    "\n",
    "            D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Generator 학습\n",
    "            fake_images = G(z, alpha, step)\n",
    "            g_loss = -D(fake_images, alpha, step).mean()\n",
    "            \n",
    "            G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            alpha += (bs / len(dataset)) * (1 / progressive_epochs[step]) * 2\n",
    "            alpha = min(alpha, 1)\n",
    "\n",
    "        # Epoch의 끝에서 fake 이미지 저장\n",
    "        save_fake_images(global_epochs, G, fixed_noise, alpha, step)\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] Global Epoch:{global_epochs} D Loss : {d_loss.item():.4f} G Loss : {g_loss.item():.4f}\")\n",
    "        global_epochs += 1  # 전체 학습에서의 epoch 수 업데이트\n",
    "\n",
    "    step += 1  # 다음 프로그레시브 학습 단계로 이동\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
