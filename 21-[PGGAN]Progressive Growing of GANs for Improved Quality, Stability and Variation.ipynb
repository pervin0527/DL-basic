{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 17:32:18.598519: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 17:32:18.672471: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-07 17:32:18.995664: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-03-07 17:32:18.995699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-03-07 17:32:18.995702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "\n",
    "if not os.path.exists(\"./runs/PGGAN\"):\n",
    "    os.makedirs(\"./runs/PGGAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module): ## Weight Standardization Convolution\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "    \n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    # x-shape: Batch Size x Channels x H X W\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(PixelNorm(),\n",
    "                                    nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    PixelNorm())\n",
    "\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = (nn.ModuleList([]), nn.ModuleList([self.initial_rgb]))\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            \n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1))\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3]))\n",
    "\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "            \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes at image size 4x4\n",
      "Succes at image size 8x8\n",
      "Succes at image size 16x16\n",
      "Succes at image size 32x32\n",
      "Succes at image size 64x64\n",
      "Succes at image size 128x128\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    Z_DIM = 100\n",
    "    IN_CHANNELS = 128\n",
    "    IMG_CHANNELS = 3\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, IMG_CHANNELS)\n",
    "    critic = Discriminator(IN_CHANNELS, IMG_CHANNELS)\n",
    "    \n",
    "    for img_size in [4,8,16,32,64,128]:\n",
    "        num_steps = int(math.log2(img_size/4))\n",
    "        z = torch.randn((1,Z_DIM,1,1))\n",
    "        generated = gen(z,0.5,steps=num_steps)\n",
    "        \n",
    "        assert generated.shape == (1, IMG_CHANNELS, img_size, img_size)\n",
    "        \n",
    "        critic_generated = critic(generated,0.5,steps=num_steps)\n",
    "        \n",
    "        assert critic_generated.shape == (1,1)\n",
    "        print(f\"Succes at image size {img_size}x{img_size}\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "START_TRAIN_IMG_SIZE = 16\n",
    "DATASET = \"/home/pervinco/Datasets/CelebA\"\n",
    "SAVE_DIR = \"./runs/PGGAN\"\n",
    "\n",
    "CHECKPOINT_GEN = f\"{SAVE_DIR}/generator.pth\"\n",
    "CHECKPOINT_CRITIC = f\"{SAVE_DIR}/critic.pth\"\n",
    "SAVE_MODEL = False\n",
    "LOAD_MODEL = False\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [32,32,32,32,16,16,16,4,4,4] ## modifiable/ Batch_sizes for each step\n",
    "IMAGE_SIZE = 128 ## 1024 for paper\n",
    "IMG_CHANNELS = 3\n",
    "Z_DIM = 256 ## 512 for paper\n",
    "IN_CHANNELS = 256 ## 512 for paper\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(math.log2(IMAGE_SIZE/4)) + 1\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [4] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8,Z_DIM,1,1).to(DEVICE)\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_tensorboard(writer,loss_critic,loss_gen,real,fake,tensorboard_step):\n",
    "    writer.add_scalar(\"Loss Critic\",loss_critic,global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"Loss Generator\", loss_gen, global_step=tensorboard_step)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8],normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8],normalize=True)\n",
    "        \n",
    "        writer.add_image(\"Real\",img_grid_real,global_step = tensorboard_step)\n",
    "        writer.add_image(\"Fake\",img_grid_fake,global_step = tensorboard_step)\n",
    "        \n",
    "def gradient_penalty(critic,real,fake,alpha,train_step,device=\"cpu\"):\n",
    "    BATCH_SIZE,C,H,W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE,1,1,1)).repeat(1,C,H,W).to(device)\n",
    "    \n",
    "    interpolated_images = real * beta + fake.detach() * (1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    ## Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images,alpha,train_step)\n",
    "    \n",
    "    ## Take the gradient of the scores with respect to the image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return penalty\n",
    "\n",
    "def save_checkpoint(model,optimizer,filename=\"my_checkpoint.pth\"):\n",
    "    print(\"Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\" : optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint,filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint_file,model,optimizer,lr):\n",
    "    print(\"Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file,map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "def generate_examples(gen,current_epoch,steps,n=16):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1,Z_DIM,1,1).to(DEVICE)\n",
    "            generated_img = gen(noise,alpha=alpha,steps=steps)\n",
    "            save_image(generated_img*0.5+0.5,f\"{SAVE_DIR}/step{steps}_epoch{current_epoch}_{i}.png\")\n",
    "                \n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:16 | Current step:2\n",
      "Epoch [1/4] Global Epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [00:47<00:00, 132.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Global Epoch:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [00:45<00:00, 138.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Global Epoch:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [00:45<00:00, 138.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4] Global Epoch:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [00:46<00:00, 137.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:32 | Current step:3\n",
      "Epoch [1/4] Global Epoch:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [01:52<00:00, 56.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Global Epoch:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [01:52<00:00, 56.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Global Epoch:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [01:52<00:00, 56.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4] Global Epoch:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6332/6332 [01:52<00:00, 56.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:64 | Current step:4\n",
      "Epoch [1/4] Global Epoch:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [07:57<00:00, 26.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Global Epoch:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [07:56<00:00, 26.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Global Epoch:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [07:56<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4] Global Epoch:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [07:56<00:00, 26.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:128 | Current step:5\n",
      "Epoch [1/4] Global Epoch:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [18:20<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Global Epoch:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [18:21<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Global Epoch:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [18:20<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4] Global Epoch:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [18:20<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:256 | Current step:6\n",
      "Epoch [1/4] Global Epoch:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [42:08<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4] Global Epoch:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12663/12663 [42:07<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4] Global Epoch:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 7579/12663 [25:13<16:51,  5.03it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "def get_loader(img_size):\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)],[0.5 for _ in range(IMG_CHANNELS)])\n",
    "    ])\n",
    "    \n",
    "    batch_size = BATCH_SIZES[int(math.log2(img_size/4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET,transform=transform)\n",
    "    loader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "    \n",
    "    return loader,dataset\n",
    "\n",
    "def train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic):\n",
    "    loop = tqdm(loader,leave=True)\n",
    "    \n",
    "    i = 0\n",
    "    for batch_idx,(real,_) in enumerate(loop):\n",
    "        i += 1\n",
    "        if i%2 == 0:\n",
    "            continue\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        noise = torch.randn(cur_batch_size,Z_DIM,1,1).to(DEVICE)\n",
    "        \n",
    "        ## Train Critic\n",
    "        ## Wasserstein Loss : Maximize \"E[Critic(real)] - E[Critic(fake)]\"   ==   Minimize \"-(E[Critic(real)] - E[Critic(fake)])\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise,alpha,step).to(DEVICE)\n",
    "            critic_real = critic(real,alpha,step)\n",
    "            critic_fake = critic(fake.detach(),alpha,step)\n",
    "            gp = gradient_penalty(critic,real,fake,alpha,step,device=DEVICE)\n",
    "            loss_critic = -1 * (torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp + 0.001 * torch.mean(critic_real**2)\n",
    "        \n",
    "        critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "        \n",
    "        ## Train Generator\n",
    "        ## Maximize \"E[Critic(fake)]\"   ==   Minimize \"- E[Critic(fake)]\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake,alpha,step)\n",
    "            loss_gen = -1 * torch.mean(gen_fake)\n",
    "            \n",
    "        gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "    \n",
    "        alpha += (cur_batch_size/len(dataset)) * (1/PROGRESSIVE_EPOCHS[step]) * 2\n",
    "        alpha = min(alpha,1)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE,alpha,step) * 0.5 + 0.5\n",
    "                save_on_tensorboard(writer,loss_critic.item(),loss_gen.item(),real.detach(),fixed_fakes.detach(),tensorboard_step)\n",
    "                tensorboard_step += 1\n",
    "    \n",
    "    return tensorboard_step,alpha\n",
    "        \n",
    "## build model\n",
    "gen = Generator(Z_DIM,IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "critic = Discriminator(IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "\n",
    "## initialize optimizer,scalers (for FP16 training)\n",
    "opt_gen = optim.Adam(gen.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "opt_critic = optim.Adam(critic.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "scaler_gen = torch.cuda.amp.GradScaler()\n",
    "scaler_critic = torch.cuda.amp.GradScaler()\n",
    "\n",
    "## tensorboard writer\n",
    "writer = SummaryWriter(f\"{SAVE_DIR}\")\n",
    "tensorboard_step = 0\n",
    "\n",
    "## if checkpoint files exist, load model\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(CHECKPOINT_GEN,gen,opt_gen,LR)\n",
    "    load_checkpoint(CHECKPOINT_CRITIC,critic,opt_critic,LR)\n",
    "    \n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "step = int(math.log2(START_TRAIN_IMG_SIZE/4)) ## starts from 0\n",
    "\n",
    "global_epoch = 0\n",
    "generate_examples_at = [4,8,12,16,20,24,28,32]\n",
    "\n",
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    alpha = 1e-4\n",
    "    loader,dataset = get_loader(4*2**step)\n",
    "    print(f\"Image size:{4*2**step} | Current step:{step}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Global Epoch:{global_epoch}\")\n",
    "        tensorboard_step,alpha = train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic)\n",
    "        global_epoch += 1\n",
    "        if global_epoch in generate_examples_at:\n",
    "            generate_examples(gen,global_epoch,step,n=6)\n",
    "        \n",
    "        if SAVE_MODEL and (epoch+1)%8==0:\n",
    "            save_checkpoint(gen,opt_gen,filename=\"CHECKPOINT_GEN\")\n",
    "            save_checkpoint(critic,opt_critic,filename=\"CHECKPOINT_CRITIC\")\n",
    "            \n",
    "    step += 1 ## Progressive Growing\n",
    "    \n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
