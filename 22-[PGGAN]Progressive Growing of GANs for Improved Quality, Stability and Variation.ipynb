{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = \"/home/pervinco/Datasets/celeba_hq_256\"\n",
    "save_dir = \"./runs/PGGAN\"\n",
    "\n",
    "lr = 0.0001\n",
    "beta1 = 0.0\n",
    "beta2 = 0.99\n",
    "batch_sizes = [256, 256, 128, 64, 16, 4] ## img_size : 4, 8, 16, 32, 64, 128\n",
    "\n",
    "nc = 3\n",
    "nz = 256\n",
    "ndf = 256\n",
    "ngf = 256\n",
    "\n",
    "min_img_size = 4\n",
    "max_img_size = 128\n",
    "gp_coeff = 10\n",
    "\n",
    "progressive_epochs = [100] * len(batch_sizes)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Equlized Learning Rate Conv2d\n",
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        학습률을 균등화하기 위해 가중치에 곱해지는 Scaling Factor.\n",
    "        계산 공식은 He 초기화 방식을 변형한 것으로, 가중치의 분산을 조절하여 특정 레이어를 통과할 때의 학습률을 균일하게 유지한다.\n",
    "        \"\"\"\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pixel Norm\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.use_pixelnorm = use_pixelnorm\n",
    "        self.conv1 = EqualConv2d(in_channels, out_channels)\n",
    "        self.conv2 = EqualConv2d(out_channels, out_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.pixel_norm = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.lrelu(x)\n",
    "        if self.use_pixelnorm:\n",
    "            x = self.pixel_norm(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.lrelu(x)\n",
    "        if self.use_pixelnorm:\n",
    "            x = self.pixel_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(PixelNorm(),\n",
    "                                    nn.ConvTranspose2d(nz, ngf, kernel_size=4, stride=1, padding=0),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    EqualConv2d(ngf, ngf, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.LeakyReLU(0.2),\n",
    "                                    PixelNorm())\n",
    "        self.toRGB1 = EqualConv2d(ngf, nc, kernel_size=1, stride=1, padding=0) ## 생성된 특성 맵을 이미지의 RGB 채널로 변환\n",
    "\n",
    "        ## 모델이 점진적으로 성장하면서 추가될 블록들과 해당 블록들로부터의 RGB 변환 레이어를 저장.\n",
    "        self.prog_blocks, self.toRGBs = (nn.ModuleList([]), nn.ModuleList([self.toRGB1]))\n",
    "        \n",
    "        ## 마지막 요소를 제외한 모든 요소를 순회\n",
    "        for i in range(len(factors) - 1):\n",
    "            in_channels = int(ngf * factors[i]) ## 128, 128, 128, 128, 64, 32, 16, 8\n",
    "            out_channels = int(ngf * factors[i + 1]) ## 128, 128, 128, 64, 32, 16, 8, 4\n",
    "            self.prog_blocks.append(ConvBlock(in_channels, out_channels))\n",
    "            self.toRGBs.append(EqualConv2d(out_channels, nc, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        ## alpha 값을 사용하여 이전 단계의 출력(upscaled)과 현재 단계의 출력(generated)을 선형적으로 혼합\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.block1(x)\n",
    "\n",
    "        ## steps == 0일 경우, 초기 단계의 출력만을 사용하여 결과를 반환.\n",
    "        if steps == 0:\n",
    "            return self.toRGB1(out)\n",
    "        \n",
    "        ## steps > 0일 경우, 각 단계마다 F.interpolate를 사용하여 출력을 업스케일링하고, \n",
    "        ## self.prog_blocks의 해당하는 블록으로 처리\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.toRGBs[steps-1](upscaled)\n",
    "        final_output = self.toRGBs[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_output)\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super().__init__()\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.prog_blocks, self.fromRGBs = nn.ModuleList([]), nn.ModuleList([])\n",
    "\n",
    "        ## 높은 해상도에서 낮은 해상도로 이동하며 판별자 블록을 초기화.\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            in_channels = int(ndf * factors[i])\n",
    "            out_channels = int(ndf * factors[i-1])\n",
    "            self.prog_blocks.append(ConvBlock(in_channels, out_channels, use_pixelnorm=True))\n",
    "            self.fromRGBs.append(EqualConv2d(nc, in_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        self.fromRGB1 = EqualConv2d(nc, ndf, kernel_size=1, stride=1, padding=0)\n",
    "        self.fromRGBs.append(self.fromRGB1)\n",
    "\n",
    "        ## 평균 풀링, stride=2이므로 크기를 절반으로 줄인다.\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(EqualConv2d(ndf + 1, ndf, kernel_size=3, padding=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         EqualConv2d(ndf, ndf, kernel_size=4, padding=0, stride=1),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         EqualConv2d(ndf, 1, kernel_size=1, padding=0, stride=1))\n",
    "    \n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        ## alpha 값을 사용하여 이전 해상도의 이미지(downscaled)와 현재 처리된 이미지(out)를 혼합.\n",
    "        return alpha * out + (1-alpha) * downscaled\n",
    "    \n",
    "    def minibatch_discrimination(self, x):\n",
    "        batch_stat = (torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3]))\n",
    "\n",
    "        return torch.cat([x, batch_stat], dim=1)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        ## 현재 처리해야 할 해상도의 단계를 결정\n",
    "        current_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        out = self.fromRGBs[current_step](x)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_discrimination(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.lrelu(self.fromRGBs[current_step+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[current_step](out))\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(current_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_discrimination(out)\n",
    "\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    bs, c, h, w = real.shape\n",
    "    beta = torch.rand((bs,1,1,1)).repeat(1,c, h,w).to(device)\n",
    "    \n",
    "    interpolated_images = real * beta + fake.detach() * (1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    ## Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images,alpha,train_step)\n",
    "    \n",
    "    ## Take the gradient of the scores with respect to the image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(img_size, batch_size):\n",
    "    dataset = datasets.ImageFolder(root=data_dir,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize(img_size),\n",
    "                                        transforms.CenterCrop(img_size),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                    ]))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def save_fake_images(epoch, G, fixed_noise, alpha, step, num_images=64):\n",
    "    with torch.no_grad():  # 그래디언트 계산을 하지 않음\n",
    "        fake_images = G(fixed_noise, alpha, step) * 0.5 + 0.5\n",
    "        fake_images = fake_images.detach().cpu()\n",
    "        img_size = fake_images.size(-1)\n",
    "        \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Fake Images at Epoch {epoch}\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(fake_images[:num_images], padding=2, normalize=True), (1, 2, 0)))\n",
    "    plt.savefig(f\"{save_dir}/Gep_{epoch}_{img_size}x{img_size}.png\")  # 이미지 파일로 저장\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:4 | Current batch size : 256 | Current step:0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [10/100] Global Epoch:10 D Loss : 0.2203 G Loss : -0.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [20/100] Global Epoch:20 D Loss : 0.1675 G Loss : -0.1965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [30/100] Global Epoch:30 D Loss : 0.1559 G Loss : -0.1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [40/100] Global Epoch:40 D Loss : 0.1535 G Loss : -0.1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [50/100] Global Epoch:50 D Loss : 0.1687 G Loss : 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [60/100] Global Epoch:60 D Loss : 0.1040 G Loss : -0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [70/100] Global Epoch:70 D Loss : 0.0807 G Loss : -0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [80/100] Global Epoch:80 D Loss : 0.0728 G Loss : 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [90/100] Global Epoch:90 D Loss : 0.0868 G Loss : 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:8 | Current batch size : 256 | Current step:1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [10/100] Global Epoch:110 D Loss : 0.0833 G Loss : 0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [20/100] Global Epoch:120 D Loss : 0.0899 G Loss : 0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [30/100] Global Epoch:130 D Loss : 0.0158 G Loss : -0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [40/100] Global Epoch:140 D Loss : 0.0334 G Loss : -0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [50/100] Global Epoch:150 D Loss : -0.0590 G Loss : -0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [60/100] Global Epoch:160 D Loss : 0.0716 G Loss : -0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [70/100] Global Epoch:170 D Loss : 0.0409 G Loss : -0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [80/100] Global Epoch:180 D Loss : 0.0238 G Loss : -0.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [90/100] Global Epoch:190 D Loss : 0.0231 G Loss : 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:16 | Current batch size : 128 | Current step:2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [10/100] Global Epoch:210 D Loss : -0.3068 G Loss : 1.5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [20/100] Global Epoch:220 D Loss : -0.2104 G Loss : 0.2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [30/100] Global Epoch:230 D Loss : -0.1912 G Loss : 0.4754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [40/100] Global Epoch:240 D Loss : -0.2531 G Loss : -0.0912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [50/100] Global Epoch:250 D Loss : -0.2930 G Loss : 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [60/100] Global Epoch:260 D Loss : -0.1231 G Loss : 0.4452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [70/100] Global Epoch:270 D Loss : -0.1049 G Loss : 0.3537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [80/100] Global Epoch:280 D Loss : -0.1768 G Loss : -0.2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [90/100] Global Epoch:290 D Loss : -0.0682 G Loss : 0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:32 | Current batch size : 64 | Current step:3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [10/100] Global Epoch:310 D Loss : 0.0921 G Loss : 1.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch [20/100] Global Epoch:320 D Loss : -0.1076 G Loss : 0.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m d_real_loss \u001b[38;5;241m=\u001b[39m D(real_images, alpha, step)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     28\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m D(fake_images\u001b[38;5;241m.\u001b[39mdetach(), alpha, step)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 29\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(d_real_loss \u001b[38;5;241m-\u001b[39m d_fake_loss) \u001b[38;5;241m+\u001b[39m gp_coeff \u001b[38;5;241m*\u001b[39m gp \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m (d_real_loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     32\u001b[0m D\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mgradient_penalty\u001b[0;34m(critic, real, fake, alpha, train_step, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient_penalty\u001b[39m(critic, real, fake, alpha, train_step, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     bs, c, h, w \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 3\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     interpolated_images \u001b[38;5;241m=\u001b[39m real \u001b[38;5;241m*\u001b[39m beta \u001b[38;5;241m+\u001b[39m fake\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbeta)\n\u001b[1;32m      6\u001b[0m     interpolated_images\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = Generator(nz, ngf, nc).to(device)\n",
    "D = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "global_epochs = 0\n",
    "step = int(math.log2(min_img_size / 4))\n",
    "fixed_noise = torch.randn(64, nz, 1, 1).to(device)\n",
    "\n",
    "# 프로그레시브 학습 단계를 위한 루프\n",
    "for n_epochs in progressive_epochs[step:]:\n",
    "    alpha = 0.00001\n",
    "    img_size = 4*2**step\n",
    "    b = int(math.log2(img_size / 4))\n",
    "    batch_size = batch_sizes[b]\n",
    "    dataset, dataloader = get_dataset(img_size=4*2**step, batch_size=batch_size)\n",
    "    print(f\"Image size:{4*2**step} | Current batch size : {batch_size} | Current step:{step}\", end=\"\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for idx, (images, _) in enumerate(tqdm(dataloader, desc=\"Train\", leave=False)):\n",
    "            bs = images.size(0)\n",
    "            real_images = images.to(device)\n",
    "            z = torch.randn(bs, nz, 1, 1).to(device)\n",
    "\n",
    "            # Discriminator 학습\n",
    "            fake_images = G(z, alpha, step)\n",
    "            d_real_loss = D(real_images, alpha, step).mean()\n",
    "            d_fake_loss = D(fake_images.detach(), alpha, step).mean()\n",
    "            gp = gradient_penalty(D, real_images, fake_images, alpha, step, device=device)\n",
    "            d_loss = -(d_real_loss - d_fake_loss) + gp_coeff * gp + 0.001 * (d_real_loss ** 2)\n",
    "\n",
    "            D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Generator 학습\n",
    "            fake_images = G(z, alpha, step)\n",
    "            g_loss = -D(fake_images, alpha, step).mean()\n",
    "            \n",
    "            G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            alpha += (bs / len(dataset)) * (1 / progressive_epochs[step]) * 2\n",
    "            alpha = min(alpha, 1)\n",
    "\n",
    "        if epoch > 0 and epoch % 10 == 0:\n",
    "            print(f\"\\tEpoch [{epoch}/{n_epochs}] Global Epoch:{global_epochs} D Loss : {d_loss.item():.4f} G Loss : {g_loss.item():.4f}\")\n",
    "            save_fake_images(global_epochs, G, fixed_noise, alpha, step)\n",
    "\n",
    "        global_epochs += 1  # 전체 학습에서의 epoch 수 업데이트\n",
    "    step += 1  # 다음 프로그레시브 학습 단계로 이동\n",
    "\n",
    "torch.save(G.state_dict(), f'{save_dir}/G.ckpt')\n",
    "torch.save(D.state_dict(), f'{save_dir}/D.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
