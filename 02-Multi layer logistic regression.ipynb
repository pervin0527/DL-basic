{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-multi layer logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.Load Dataset](#1)\n",
    "\n",
    "[2.Build Parts of NN](#2)\n",
    "  - [2-1.forward propagation](#2-1)  \n",
    "  - [2-2.Compute Cost](#2-2)  \n",
    "  - [2-3.Back Propagation](#2-3)  \n",
    "  - [2-4.Update Parameters](#2-4)  \n",
    "\n",
    "[3.Build Integrated NN](#3)\n",
    "\n",
    "[4.Predict](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=5000, noise=0.2, random_state=42)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (2, 4000)\n",
      "train_y shape: (1, 4000)\n",
      "test_x shape: (2, 1000)\n",
      "test_y shape: (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_y = train_y.reshape(-1, 1)\n",
    "test_y = test_y.reshape(-1, 1)\n",
    "\n",
    "train_x = train_x.T\n",
    "train_y = train_y.T\n",
    "test_x = test_x.T\n",
    "test_y = test_y.T\n",
    "\n",
    "print(\"train_x shape:\", train_x.shape)\n",
    "print(\"train_y shape:\", train_y.shape)\n",
    "print(\"test_x shape:\", test_x.shape)\n",
    "print(\"test_y shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Build Parts of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n",
      "(1, 4) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "n_x, n_h, n_y = layer_sizes(train_x, train_y)\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(parameters[\"W1\"].shape, parameters[\"b1\"].shape)\n",
    "print(parameters[\"W2\"].shape, parameters[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-1\"></a>\n",
    "### 2-1. Forwad Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]} \\tag{1}$$ \n",
    "$$a^{[1](i)} = \\tanh(z^{[1](i)}) \\tag{2}$$\n",
    "$$z^{[2](i)} = W^{[2]} a^{[1](i)} + b^{[2]} \\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2](i)} = \\sigma(z^{[2](i)}) \\tag{4}$$\n",
    "$$y^{(i)}_{\\text{prediction}} = \\begin{cases} 1 & \\text{if } a^{[2](i)} > 0.5 \\\\ 0 & \\text{otherwise} \\end{cases} \\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "\n",
    "    cache = {\"Z1\" : Z1, \"A1\" : A1, \"Z2\" : Z2, \"A2\" : A2}\n",
    "\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4000)\n"
     ]
    }
   ],
   "source": [
    "A2, cache = forward_propagation(train_x, parameters)\n",
    "print(A2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2\"></a>\n",
    "### 2-2. Compute Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1-A2)) ## loss function\n",
    "    cost = -np.sum(loss) / m ## cost function\n",
    "\n",
    "    # cost = float(np.squeeze(cost))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931103065067663\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(A2, train_y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-3\"></a>\n",
    "### 2-3. BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 출력층의 오차:\n",
    "   $$ \\delta^{[2](i)} = \\hat{y}^{(i)} - y^{(i)} $$\n",
    "\n",
    "2. 출력층 가중치의 그래디언트:\n",
    "   $$ \\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\delta^{[2](i)} a^{[1](i)T} $$\n",
    "\n",
    "3. 출력층 편향의 그래디언트:\n",
    "   $$ \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} = \\delta^{[2](i)} $$\n",
    "\n",
    "4. 첫 번째 층의 오차:\n",
    "   $$ \\delta^{[1](i)} = (W^{[2]T} \\delta^{[2](i)}) * (1 - (a^{[1](i)})^2) $$\n",
    "\n",
    "5. 첫 번째 층 가중치의 그래디언트:\n",
    "   $$ \\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} = \\delta^{[1](i)} x^{(i)T} $$\n",
    "\n",
    "6. 첫 번째 층 편향의 그래디언트:\n",
    "   $$ \\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} = \\delta^{[1](i)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\" : dW1, \"db1\" : db1, \"dW2\" : dW2, \"db2\" : db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n",
      "(1, 4)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(parameters, cache, train_x, train_y)\n",
    "\n",
    "print(grads[\"dW1\"].shape)\n",
    "print(grads[\"db1\"].shape)\n",
    "print(grads[\"dW2\"].shape)\n",
    "print(grads[\"db2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-4\"></a>\n",
    "### 2-4. Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n",
      "(1, 4)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads)\n",
    "print(parameters[\"W1\"].shape)\n",
    "print(parameters[\"b1\"].shape)\n",
    "print(parameters[\"W2\"].shape)\n",
    "print(parameters[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Build Integrated NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 1000, print_cost=True):\n",
    "    np.random.seed(3)\n",
    "    n_x, n_h, n_y,  = layer_sizes(X, Y)\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i} : {cost}\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 : 0.6931580212201081\n",
      "Cost after iteration 100 : 0.2937521294520586\n",
      "Cost after iteration 200 : 0.2927814873086575\n",
      "Cost after iteration 300 : 0.29219237636363016\n",
      "Cost after iteration 400 : 0.29179809019473474\n",
      "Cost after iteration 500 : 0.2915138127613999\n",
      "Cost after iteration 600 : 0.29129755744484653\n",
      "Cost after iteration 700 : 0.2911264648923138\n",
      "Cost after iteration 800 : 0.29297486079499957\n",
      "Cost after iteration 900 : 0.2929205654417771\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(train_x, train_y, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 >= 0.5).astype(np.int32)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.375%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_888923/2236656979.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  accuracy = float((np.dot(train_y, predictions.T) + np.dot(1 - train_y, 1 - predictions.T)) / float(train_y.size) * 100)\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, train_x)\n",
    "accuracy = float((np.dot(train_y, predictions.T) + np.dot(1 - train_y, 1 - predictions.T)) / float(train_y.size) * 100)\n",
    "print (f'Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
