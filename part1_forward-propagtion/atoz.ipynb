{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMebuZpvv/GtFGXu+Ru75ZO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1.Affine Transform"],"metadata":{"id":"nqTOylcZF2Gn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T81t7hSFi6H"},"outputs":[],"source":["## 1개 데이터, 1개의 값에 대한 Affine Transform.\n","import tensorflow as tf\n","\n","x = tf.constant([[10.]]) ## vector([10.])가 아니라 matrix([[10.]]) 형태임에 유의할 것. shape = (1, 1)\n","\n","dense = tf.keras.layers.Dense(units=1, activation=\"linear\") ## affine function\n","\n","y = dense(x) ## output. forward propagation + parameter initialization(x값이 입력되는 시점에 weight, bias가 초기화 된다.)\n","\n","W, B = dense.get_weights()\n","\n","## Manual\n","y_manual = tf.linalg.matmul(x, W) + B\n","\n","## print reuslt\n","print(f\"x : {x.shape}, {x.numpy()}\")\n","print(f\"W : {W.shape}, {W}\")\n","print(f\"B : {B.shape}, {B}\")\n","print(f\"y : {y.shape}, {y.numpy()}\")\n","print(f\"y manual : {y_manual.shape}, {y_manual}\")"]},{"cell_type":"code","source":["## weight, bias Initializer\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense\n","\n","x = tf.constant([[1., 4., 7.]]) ## (1, 3) row vector\n","\n","dense = Dense(units=1, activation=\"linear\") ## (3, 1) column vector\n","y = dense(x) ## 1 by 1\n","\n","W, b = dense.get_weights()\n","print(f\"input : {x.shape} \\n {x} \\n\")\n","print(f\"Weight : {W.shape} \\n {W} \\n\")\n","print(f\"bias : {b.shape} \\n {b} \\n\")\n","print(f\"output : {y.shape} \\n {y}\")"],"metadata":{"id":"I23fL8VWFqyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 한 개의 row vector data. 10개의 feature.\n","x = tf.random.uniform(shape=(1, 10), minval=0, maxval=10)\n","\n","dense = tf.keras.layers.Dense(units=1)\n","y = dense(x)\n","W, B = dense.get_weights()\n","\n","y_manual = tf.linalg.matmul(x, W) + B\n","\n","## print reuslt\n","print(f\"x : {x.shape} \\n {x.numpy()}\") ## row vector\n","print(f\"W : {W.shape} \\n {W}\") ## column vector\n","print(f\"B : {B.shape} \\n {B}\")\n","print(f\"y : {y.shape} \\n {y.numpy()}\")\n","print(f\"y manual : {y_manual.shape}, {y_manual}\")"],"metadata":{"id":"bwW6HxH2F0Ju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.Mini-batch"],"metadata":{"id":"s5qeos4NF9-q"}},{"cell_type":"code","source":["## Affine Transform + Activation function with Mini-batch\n","N, n_feature = 3, 5\n","x = tf.random.normal(shape=(N, n_feature))\n","\n","dense = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n","y = dense(x)\n","\n","## Mini-batch size는 weight, bias에 영향을 끼치지 않는다!!!\n","W, B = dense.get_weights()\n","\n","print(f\"x : {x.shape} \\n {x.numpy()}\")\n","print(f\"W : {W.shape} \\n {W}\")\n","print(f\"B : {B.shape} \\n {B}\")\n","print(f\"y : {y.shape} \\n {y.numpy()}\")\n","\n","y_man = tf.linalg.matmul(x, W) + B\n","y_man = 1 / (1 + tf.math.exp(-y_man))\n","\n","print(f\"y tensorflow: {y.shape} \\n {y.numpy()} \\n\")\n","print(f\"y manual : {y_man.shape} \\n{y_man.numpy()}\")"],"metadata":{"id":"3l6lZJ_vGAAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.Layer"],"metadata":{"id":"Pvx4Nee-GWl_"}},{"cell_type":"code","source":["## Mini-batch input과 multiple neurons\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.initializers import Constant, RandomUniform\n","\n","# w, b = tf.constant(2.), tf.constant(1.)\n","# w_init, b_init = Constant(w), Constant(b)\n","w_init, b_init = RandomUniform(minval=0, maxval=3), RandomUniform(minval=0, maxval=3)\n","\n","X = tf.random.uniform(shape=(3, 3), minval=0, maxval=3, dtype=tf.int32)\n","dense = tf.keras.layers.Dense(units=4, \n","                              activation=\"linear\", \n","                              kernel_initializer=w_init, \n","                              bias_initializer=b_init)\n","Y = dense(X)\n","\n","W, B = dense.get_weights() ## Bias도 뉴런의 수만큼 생성된다.\n","\n","print(f\"Input : {X.shape} \\n {X.numpy()} \\n\")\n","print(f\"Weight : {W.shape} \\n {W} \\n\")\n","print(f\"Bias : {B.shape} \\n {B} \\n\")\n","print(f\"Output : {Y.shape} \\n {Y.numpy()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnAqklFuGV-1","executionInfo":{"status":"ok","timestamp":1677137209860,"user_tz":-540,"elapsed":8,"user":{"displayName":"김민준","userId":"08346406018640900317"}},"outputId":"4898848f-82e4-48a8-ed83-7976d69369b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input : (3, 3) \n"," [[0 2 0]\n"," [0 1 0]\n"," [1 2 0]] \n","\n","Weight : (3, 4) \n"," [[2.5851223  2.7393     0.8288884  0.766943  ]\n"," [1.8277302  0.34176064 0.86527026 2.2301023 ]\n"," [0.7015815  2.6197593  2.0963602  0.47183955]] \n","\n","Bias : (4,) \n"," [2.8081748  0.59560454 1.3720304  1.5374794 ] \n","\n","Output : (3, 4) \n"," [[6.4636354 1.2791258 3.102571  5.997684 ]\n"," [4.6359053 0.9373652 2.2373006 3.7675817]\n"," [9.048758  4.018426  3.9314594 6.764627 ]]\n"]}]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=download&id=1VV73NLtIyYKUMzybk8bzJm3Rk1Bvn-jH\">"],"metadata":{"id":"TewCSHGrMXkG"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=download&id=1BlokXqSS2wGRik0bUv5v6GqWf4TqozXs\">\n","\n","연산 과정에 대해서 확실히 파악할 것."],"metadata":{"id":"_5jujMhqKzYo"}},{"cell_type":"code","source":["## Manual\n","import numpy as np\n","import tensorflow as tf\n","\n","N, n_feature = 4, 5\n","X = tf.random.normal(shape=(N, n_feature))\n","\n","n_neuron = 3\n","dense = tf.keras.layers.Dense(units=n_neuron, activation=\"sigmoid\")\n","Y_tf = dense(X)\n","\n","W, B = dense.get_weights()\n","print(f\"Input : {X.shape} \\n {X.numpy()} \\n\")\n","print(f\"Weights : {W.shape} \\n {W} \\n\")\n","print(f\"Bias : {B.shape} \\n {B} \\n\")\n","\n","## calculate with matrix multiplication.\n","Z = tf.linalg.matmul(X, W) + B\n","Y_man_mat_mul = 1 / (1 + tf.math.exp(-Z))\n","\n","## calculate with dot product.\n","Y_man_vec = np.zeros(shape=(N, n_neuron))\n","for x_idx in range(N):\n","  x = X[x_idx] ## row 단위로 접근.\n","\n","  for neuron_idx in range(n_neuron):\n","    w, b = W[:, neuron_idx], B[neuron_idx] ## W[:, neuron_idx] column 단위로 접근.\n","    z = tf.reduce_sum(x * w) + b\n","    a = 1 / (1 + np.exp(-z))\n","    Y_man_vec[x_idx, neuron_idx] = a\n","\n","\n","print(f\"Y tensorflow : {Y_tf.shape}, \\n {Y_tf.numpy()} \\n\")\n","print(f\"Y man mat_mul : {Y_man_mat_mul.shape}, \\n {Y_man_mat_mul} \\n\")\n","print(f\"Y man vec : {Y_man_vec.shape} \\n {Y_man_vec} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRDEHqV-G-RD","executionInfo":{"status":"ok","timestamp":1677136390067,"user_tz":-540,"elapsed":386,"user":{"displayName":"김민준","userId":"08346406018640900317"}},"outputId":"2565999e-9a73-4acd-da8a-aab2e7e0a639"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input : (4, 5) \n"," [[-0.65928537 -0.2480974   0.43367472  1.3678365  -0.1264271 ]\n"," [ 0.17658404 -0.09584612 -1.0973     -0.40444714  0.23457985]\n"," [-0.00544956  0.70416385  1.2003565  -1.6772876   0.97837424]\n"," [-0.01508621  0.75583154  1.0752207  -0.82584643 -0.1614769 ]] \n","\n","Weights : (5, 3) \n"," [[-0.33535814 -0.76547253 -0.06815624]\n"," [ 0.33101016  0.7737184   0.52242225]\n"," [-0.16563344 -0.20004046  0.2815867 ]\n"," [-0.60301554  0.18159968 -0.19174463]\n"," [-0.26630932  0.00806332 -0.2891037 ]] \n","\n","Bias : (3,) \n"," [0. 0. 0.] \n","\n","Y tensorflow : (4, 3), \n"," [[0.32650948 0.6161726  0.45306733]\n"," [0.5676383  0.48465988 0.41063043]\n"," [0.68718123 0.50304246 0.67808956]\n"," [0.649799   0.5572443  0.7117081 ]] \n","\n","Y man mat_mul : (4, 3), \n"," [[0.32650945 0.61617255 0.45306733]\n"," [0.5676383  0.4846599  0.41063043]\n"," [0.68718123 0.5030425  0.67808956]\n"," [0.649799   0.55724436 0.7117082 ]] \n","\n","Y man vec : (4, 3) \n"," [[0.32650944 0.61617258 0.45306734]\n"," [0.56763829 0.48465988 0.41063043]\n"," [0.68718124 0.50304253 0.67808955]\n"," [0.64979896 0.55724434 0.71170811]] \n","\n"]}]},{"cell_type":"code","source":["## for문을 이용해서 layer들 사용하는 방법.\n","\n","N, n_feature = 4, 10\n","X = tf.random.normal(shape=(N, n_feature))\n","X_copy = tf.identity(X)\n","\n","n_neurons = [3, 4, 5]\n","dense_layers = list()\n","for n_neuron in n_neurons:\n","  dense = tf.keras.layers.Dense(units=n_neuron, activation=\"sigmoid\")\n","  dense_layers.append(dense)\n","\n","print(f\"Input : {X.shape}\")\n","\n","W, B = list(), list()\n","for dense_idx, dense in enumerate(dense_layers):\n","  X = dense(X)\n","  w, b = dense.get_weights()\n","\n","  W.append(w), B.append(b)\n","\n","print(f\"Y tf : \\n {X.numpy()}\")\n","\n","\n","for layer_idx in range(len(n_neurons)):\n","  w, b = W[layer_idx], B[layer_idx]\n","  X_copy = tf.linalg.matmul(X_copy, w) + b\n","  X_copy = 1 / (1 + tf.math.exp(-X_copy))\n","\n","print(f\"Y man : \\n {X_copy.numpy()}\")"],"metadata":{"id":"CpBiAd3-HCVI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4.Sigmoid function\n","\n","- weight, bias를 가지지 않음.\n","- output layer 직전까지의 모든 Layer가 Affine transform연산을 수행한다고 가정.\n","- Logit vector를 입력으로 받에 Probability로 출력.\n","- output layer는 뉴런이 1개(units=1)이며 activation = \"sigmoid\"로 설정.\n","- 뉴런이 1개이므로 output shape = 1 또는 (N, 1).\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1z4DekNoW_ty2W80MIrkmrsoEAcDV_094\">"],"metadata":{"id":"6aZru7ZhX19y"}},{"cell_type":"code","source":["## Logistic Regression - Multi Variate\n","X = tf.random.normal(shape=(100, 5))\n","dense = tf.keras.layers.Dense(units=1, activation=\"sigmoid\") ## Affine transform으로 1개의 logit 값을 구하고, sigmoid 반영.\n","Y = dense(X)\n","\n","W, B = dense.get_weights()\n","\n","print(f\"Input : {X.shape} \\n {X.numpy()[:3]} \\n\")\n","print(f\"Weights : {W.shape} \\n {W} \\n\")\n","print(f\"Bias : {B.shape} \\n {B} \\n\")\n","print(f\"Output - Sigmoid : {Y.shape} \\n {Y.numpy()[:3]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEVGjJ3hbWvK","executionInfo":{"status":"ok","timestamp":1677141739829,"user_tz":-540,"elapsed":458,"user":{"displayName":"김민준","userId":"08346406018640900317"}},"outputId":"a5de4d9b-3e74-4ecf-f145-dac69198d10b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input : (100, 5) \n"," [[ 0.07885586  1.2067411   0.69738185  1.1121552   0.17453901]\n"," [-0.05307293  0.19124863 -1.4629031  -0.2412481  -0.24293447]\n"," [-0.07027663  0.7556984  -1.175209    0.39808595  0.9123695 ]] \n","\n","Weights : (5, 1) \n"," [[ 0.7188616 ]\n"," [-0.02032185]\n"," [-0.46848083]\n"," [-0.30328465]\n"," [-0.66729736]] \n","\n","Bias : (1,) \n"," [0.] \n","\n","Output - Sigmoid : (100, 1) \n"," [[0.3211884 ]\n"," [0.7065278 ]\n"," [0.43908694]]\n"]}]},{"cell_type":"code","source":["## Binary Classifier with Dense Layers\n","\n","X = tf.random.normal(shape=(100, 5))\n","\n","model = tf.keras.models.Sequential()\n","# model.add(tf.keras.layers.Dense(units=10, activation=\"relu\"))\n","# model.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n","# model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\")) ## Binary classifier\n","\n","model.add(tf.keras.layers.Dense(units=10, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(units=1, activation=\"relu\"))\n","model.add(tf.keras.layers.Activation(\"sigmoid\"))\n","\n","Y = model(X)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ePzZEajcYga","executionInfo":{"status":"ok","timestamp":1677142204582,"user_tz":-540,"elapsed":499,"user":{"displayName":"김민준","userId":"08346406018640900317"}},"outputId":"adbec89b-49c0-4e8c-f4d3-d7a92349b210"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_14 (Dense)            (100, 10)                 60        \n","                                                                 \n"," dense_15 (Dense)            (100, 5)                  55        \n","                                                                 \n"," dense_16 (Dense)            (100, 1)                  6         \n","                                                                 \n"," activation (Activation)     (100, 1)                  0         \n","                                                                 \n","=================================================================\n","Total params: 121\n","Trainable params: 121\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# 5.Softmax function\n","\n","- sigmoid와 마찬가지로 weight, bias를 가지지 않음.\n","- logit vector를 입력으로 받고, probability 형태로 바꿔준다.\n","- 이 때, vector의 각 원소는 각 class에 해당할 것이라는 확률값.\n","- probability vector의 총 합이 1이다.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1zLjDO-MjmbK74_I-zWXRScTr-I0MYecr\">\n","<img src=\"https://drive.google.com/uc?export=download&id=11-c1XGGcunjid7hyI1VwuvWWl_cKkaQ4\">"],"metadata":{"id":"sbWvw0uCaQaj"}},{"cell_type":"code","source":["## Softmax\n","logit = tf.random.uniform(shape=(5, 3), minval=-10, maxval=10)\n","softmax_value = tf.keras.layers.Activation(\"softmax\")(logit)\n","softmax_sum = tf.reduce_sum(softmax_value, axis=1)\n","\n","print(f\"Logits : {logit.shape} \\n {logit} \\n\")\n","print(f\"Probabilities : {softmax_value.shape} \\n {softmax_value} \\n\")\n","print(f\"Sum of softmax values : \\n {softmax_sum}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlLIsKQTaAAi","executionInfo":{"status":"ok","timestamp":1677142301028,"user_tz":-540,"elapsed":7,"user":{"displayName":"김민준","userId":"08346406018640900317"}},"outputId":"2b3dc04b-4b14-4b0b-e46f-a78657cf67b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits : (5, 3) \n"," [[ 0.88398457 -3.945303   -7.8322077 ]\n"," [ 4.4274254   4.337284   -6.25705   ]\n"," [ 5.1670504   2.4240227  -7.380731  ]\n"," [-6.0878897   2.852028    4.2614384 ]\n"," [-5.7236433  -5.4685473  -4.6333027 ]] \n","\n","Probabilities : (5, 3) \n"," [[9.9190980e-01 7.9275547e-03 1.6258407e-04]\n"," [5.2251381e-01 4.7747421e-01 1.1964355e-05]\n"," [9.3951517e-01 6.0481429e-02 3.3378881e-06]\n"," [2.5728361e-05 1.9632201e-01 8.0365229e-01]\n"," [1.8990204e-01 2.4508482e-01 5.6501317e-01]] \n","\n","Sum of softmax values : \n"," [0.99999994 1.         0.99999994 1.         1.        ]\n"]}]},{"cell_type":"code","source":["## Softmax in Dense Layers\n","logit = tf.random.uniform(shape=(8, 5), minval=-10, maxval=10)\n","dense = tf.keras.layers.Dense(units=8, activation=\"softmax\")\n","Y = dense(logit)\n","print(tf.reduce_sum(Y, axis=1))"],"metadata":{"id":"wRUB2-SVd7cB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Multiclass classifiers\n","\n","class TestModel(tf.keras.models.Model):\n","    def __init__(self):\n","        super(TestModel, self).__init__()\n","\n","        self.dense1 = tf.keras.layers.Dense(units=8, activation=\"relu\")\n","        self.dense2 = tf.keras.layers.Dense(units=5, activation=\"relu\")\n","        self.dense3 = tf.keras.layers.Dense(units=3, activation=\"softmax\") ## 3 classes\n","\n","    def call(self, x):\n","        print(f\"X : {x.shape} \\n {x.numpy()} \\n\")\n","\n","        x = self.dense1(x)\n","        print(f\"A1 : {x.shape} \\n {x.numpy()} \\n\")\n","\n","        x = self.dense2(x)\n","        print(f\"A2 : {x.shape} \\n {x.numpy()} \\n\")\n","\n","        x = self.dense3(x)\n","        print(f\"Y : {x.shape} \\n {x.numpy()} \\n\")\n","        print(f\"Sum of Y : {x.shape} \\n {tf.reduce_sum(x, axis=1)}\")\n","\n","model = TestModel()\n","X = tf.random.uniform(shape=(8, 5), minval=-10, maxval=10)\n","Y = model(X)\n"],"metadata":{"id":"YmEm3dc7eagt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6.Loss functions"],"metadata":{"id":"k91scwjGe5XV"}},{"cell_type":"markdown","source":["## 6-1.Mean Squared Erros\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1YHlQlRMG3g3vHQgkKmq_PAxCBm4g6nSI\">\n","\n","- 입력 데이터 X는 l<sub>i</sub> 개의 feature를 가진 row vector가 N개 쌓여있는 Matrix.\n","- 학생1, 학생2, ... 들의 데이터가 batch size N만큼 존재.\n","- 데이터 row 각각에 대한 prediction value를 구하고, column vector $\\hat{Y}$로 나타낸다.\n","- 정답값 y들이 vector로 묶인 column vector Y와 비교를 진행한다.\n","- batch-size N개에 대한 비교이므로 이에 대한 평균을 구함. Mean Squared Error.\n","- 평균을 구하는 이유는 N개의 y와 prediction 간의 차이를 평균 낸 것이 바로 loss 값."],"metadata":{"id":"PvFPrYgbfDHN"}},{"cell_type":"code","source":["## Regression dataset 만들기\n","\n","import tensorflow as tf\n","\n","N, n_feature = 8, 5\n","X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature)) ## 8 by 5 matrix\n","target_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32) ## 5 by 1 column\n","target_bias = tf.constant([10], dtype=tf.float32) ## 1 bias\n","\n","print(X.shape, target_weights.shape, target_bias.shape)\n","\n","Y = tf.reduce_sum(X * target_weights, axis=1) + target_bias\n","print(f\"X : {X.shape} \\n {X}\")\n","print(f\"Y : {Y.shape} \\n {Y}\")\n","\n","## Mean Squared Error\n","\n","loss_object = tf.keras.losses.MeanSquaredError()\n","\n","batch_size = 32\n","predictions = tf.random.normal(shape=(batch_size, 1))\n","labels = tf.random.normal(shape=(batch_size, 1))\n","\n","mse = loss_object(labels, predictions)\n","mse_man = tf.reduce_mean(tf.math.pow(labels - predictions, 2))\n","print(f\"MSE tensorflow : {mse}\")\n","print(f\"MSE manual : {mse_man}\")"],"metadata":{"id":"HIHOxfD2fCnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Mean Squared Error with model\n","\n","batch_size = 32\n","N, n_features = 100, 5\n","X = tf.random.normal(shape=(N, n_feature))\n","Y = tf.random.normal(shape=(N, 1))\n","\n","dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n","dataset = dataset.batch(batch_size)\n","\n","dense = tf.keras.layers.Dense(units=1, activation=\"linear\")\n","loss_object = tf.keras.losses.MeanSquaredError()\n","\n","for x, y in dataset:\n","    predictions = dense(x)\n","    loss = loss_object(y, predictions)\n","    print(loss.numpy())"],"metadata":{"id":"ap0A3o70fr01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6-2.Binary Cross Entropy\n","<img src=\"https://drive.google.com/uc?export=download&id=1UElDXIkw1Y3HTgrariXNBheUCqvuV2A8\">\n","\n","- y = 1 인 경우, $(1-y)log(1-\\hat{y}) = 0$이 된다. 따라서 $-log(\\hat{y})$ 항만 남음.\n","- -log 함수는 입력이 1에 가까워질수록 출력이 0에 수렴하고, 입력이 0에 가까워질수록 출력이 무한히 발산한다.\n","\n","- y = 0 인경우, $ylog(\\hat{y})$ 항이 0이 된다. 따라서 $-log(1-\\hat{y})$ 항만 남음.\n","- $-log(1-\\hat{y})$ 함수는 입력값이 1에 가까워질수록 함수값은 무한히 발산. 입력값이 0에 가까워질수록 함수값은 0에 수렴한다."],"metadata":{"id":"nlm6Ph5Ef7PY"}},{"cell_type":"code","source":["## Binary Classification\n","import tensorflow as tf\n","\n","N, n_feature = 8, 5\n","target_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n","target_bias = tf.constant([10], dtype=tf.float32)\n","\n","X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature))\n","print(f\"X : {X.shape} \\n {X}\")\n","print(X.shape, target_weights.shape, target_bias.shape)\n","\n","Y = tf.reduce_sum(X * target_weights, axis=1) + target_bias\n","print(f\"reduce sum : {Y.shape} \\n {Y.numpy()}\")\n","\n","Y = tf.cast(Y > 5, tf.int32) ## 5 이상이면 True, 미만이면 False.\n","print(f\"Y : {Y.shape} \\n {Y}\")"],"metadata":{"id":"bBMTDPm8gh5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Binary Cross Entropy\n","batch_size = 10\n","n_class = 2\n","\n","predictions = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=1, dtype=tf.float32)\n","labels = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=n_class, dtype=tf.int32)\n","\n","print(predictions) ## 10 by 1\n","print(labels) ## 10 by 1\n","\n","loss_object = tf.keras.losses.BinaryCrossentropy()\n","loss = loss_object(labels, predictions)\n","\n","labels = tf.cast(labels, tf.float32)\n","bce_man = -(labels * tf.math.log(predictions) + (1 - labels) * tf.math.log(1 - predictions))\n","bce_man = tf.reduce_mean(bce_man)\n","\n","print(f\"BCE : {loss}\")\n","print(f\"BCE man : {bce_man}\")"],"metadata":{"id":"V8GMQ43fgx36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Binary Cross Entropy with model\n","\n","N, n_feature = 8, 5\n","target_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32) ## \n","target_bias = tf.constant([0], dtype=tf.float32)\n","\n","X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature)) ## 8 by 5 matrix\n","Y = tf.reduce_sum(target_weights * X, axis=1) + target_bias ## affine transform 8 by 1\n","Y = tf.cast(Y > 5, tf.int32) ## 8 by 1\n","\n","print(f\"X : {X.shape} \\n {X}\")\n","print(f\"Y : {Y.shape} \\n {Y}\")\n","\n","dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n","dataset = dataset.batch(batch_size)\n","\n","dense = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n","loss_object = tf.keras.losses.BinaryCrossentropy()\n","\n","for x, y in dataset:\n","    predictions = dense(x)\n","    print(f\"predictions : {predictions.shape} \\n {predictions}\")\n","    loss = loss_object(y, predictions)\n","    print(loss.numpy())"],"metadata":{"id":"oHpoF0lTg7Br"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##6-3. Categorical CrossEntropy\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1s2i-DLyj7hyAJIYFXVQtKnqR18bVqQ4_\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1RAKAG5SNIqS7REHgqW3XhgNhtulubyBJ\">\n","\n","- index가 1부터 k까지 증가하면서 각각의 $-log{(\\hat{y})}$ 값을 구함.\n","- 구한 loss 값을 다 더하면 한 개의 y vector와 prediction vector에 대한 차이. 오차.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1OWpDF4PlSOnyGfLBQYI3V8RB8FW9okp2\">\n","- 전체 prediction matrix와 y matrix 간의 평균."],"metadata":{"id":"1TgWz1gbhELo"}},{"cell_type":"markdown","source":["# 7.Convolutional layer\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1L_RYA5LZsmd1psRgMy44c_cGNLGh2Co5\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1YsW8dXtvmbKLsWN0EqwMBr3AJdBg4DPd\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1sWbSmhtTtxDypIyvV8s6ATrIeycqj2p9\">\n","\n","\n","- Dense layer처럼 모든 입력이 한 뉴런에 입력되는 형태가 아님.\n","- window라는 개념이 도입되어 input을 순차적으로 정해진 규격만큼 뽑아낸다.\n","- window 크기와 동일하게 kernel 규격이 만들어지고, window와 kernel간 correlation 연산을 수행한다.\n","- correlation은 window와 kernel간 element-wise product.\n","- element wise product 한 값을 모두 더하고, bias 값까지 더해주면 output matrix의 원소값이 된다.\n","- output matrix는 window가 순회하면서 만들어짐.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1Sqh6yrWp_XJ5MtH-GsDwmP5VdKChIplK\">"],"metadata":{"id":"2YX9ZY5kirbl"}},{"cell_type":"code","source":["## Gray Scale image\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","N, n_H, n_W, n_C = 1, 28, 28, 1\n","n_filter = 1 ## num filter\n","k_size = 3 ## kernel size\n","\n","images = tf.random.uniform(minval=0, maxval=1, shape=((N, n_H, n_W, n_C)))\n","\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","y = conv(images)\n","W, B = conv.get_weights()\n","\n","print(images.shape)\n","print(W.shape) ## H, W, C, N_filter(뉴런의 수)\n","print(B.shape)\n","print(y.shape) ## 28 - 3 + 1"],"metadata":{"id":"audx24oim0lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## numpy manual\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","N, n_H, n_W, n_C = 1, 5, 5, 1\n","n_filter = 1 ## num filter\n","k_size = 3 ## kernel size\n","\n","images = tf.random.uniform(minval=0, maxval=1, shape=((N, n_H, n_W, n_C)))\n","\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","y = conv(images)\n","W, B = conv.get_weights()\n","\n","### manual\n","images = images.numpy().squeeze()\n","W = W.squeeze()\n","\n","y_man = np.zeros(shape=(n_H - k_size + 1, n_W - k_size + 1))\n","for i in range(n_H - k_size + 1):\n","    for j in range(n_W - k_size + 1):\n","        # print(i, j)\n","        window = images[i : i+k_size, j : j+k_size]\n","        y_man[i, j] = np.sum(window * W) + B\n","\n","print(images.shape)\n","print(W.shape)\n","print(B.shape)\n","\n","print(f\"y tensorflow : {y.shape} \\n {y.numpy().squeeze()}\")\n","print(f\"y man : {y_man.shape} \\n {y_man}\")"],"metadata":{"id":"bbD7BSW0m5jJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##7-1.Color Image\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1IBpSFqeZHHQy6gtF_cMhEyvGuRUuWQbt\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1NMTeDBOWz-4pnhzfaz5lLIyH06Zt9pNy\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1C0sqxnwFYnYRgLp88rgT4szZ6uVSXz75\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1uY4RVzAg7iJ_za9DBUNjLZgwm0fBBmKj\">\n","\n","- convolution layer의 filter의 수는 dense layer의 neuron의 수와 같다.(l<sub>i</sub>개의 뉴런(filter, kernel)을 배치했다.)\n","- 즉, output channel의 수가 filter의 수로 결정된다."],"metadata":{"id":"oMGlJNGUlew7"}},{"cell_type":"code","source":["## RGB Scale image\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","N, n_H, n_W, n_C = 1, 28, 28, 3\n","n_filter = 1 ## num filter\n","k_size = 3 ## kernel size\n","\n","images = tf.random.uniform(minval=0, maxval=1, shape=((N, n_H, n_W, n_C)))\n","\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","y = conv(images)\n","W, B = conv.get_weights()\n","\n","print(images.shape)\n","print(W.shape) ## H, W, C, N_filter(뉴런의 수)\n","print(B.shape)\n","print(y.shape) ## 28 - 3 + 1"],"metadata":{"id":"XPknsbV9iQqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## numpy manual color image\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","N, n_H, n_W, n_C = 1, 5, 5, 3\n","n_filter = 1 ## num filter\n","k_size = 3 ## kernel size\n","\n","images = tf.random.uniform(minval=0, maxval=1, shape=((N, n_H, n_W, n_C)))\n","\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","y = conv(images)\n","W, B = conv.get_weights()\n","\n","### manual\n","images = images.numpy().squeeze()\n","W = W.squeeze()\n","\n","y_man = np.zeros(shape=(n_H - k_size + 1, n_W - k_size + 1))\n","for i in range(n_H - k_size + 1):\n","    for j in range(n_W - k_size + 1):\n","        # print(i, j)\n","        window = images[i : i+k_size, j : j+k_size, :]\n","        y_man[i, j] = np.sum(window * W) + B\n","\n","print(images.shape)\n","print(W.shape)\n","print(B.shape)\n","\n","print(f\"y tensorflow : {y.shape} \\n {y.numpy().squeeze()}\")\n","print(f\"y man : {y_man.shape} \\n {y_man}\")"],"metadata":{"id":"ubF5_ou0m8T2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## convolution multiple kernels\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","batch_size, height, width, channel = 10, 28, 28, 3\n","n_filter = 5\n","k_size = 3\n","\n","image = tf.random.uniform(minval=0, maxval = 1, shape=(batch_size, height, width, channel))\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","Y = conv(image)\n","\n","W, B = conv.get_weights()\n","\n","print(image.shape)\n","print(W.shape, B.shape)\n","print(Y.shape)"],"metadata":{"id":"fSH2xmABnAxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## multiple kernel conv layer + manual\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","batch_size, height, width, channel = 1, 5, 5, 3\n","n_filter = 5\n","k_size = 3\n","\n","image = tf.random.uniform(minval=0, maxval = 1, shape=(batch_size, height, width, channel))\n","\n","## Tensorflow\n","conv = Conv2D(filters=n_filter, kernel_size=k_size)\n","Y = conv(image)\n","Y = np.transpose(Y.numpy().squeeze(), (2, 0, 1))\n","print(f\"Y tensorflow : {Y.shape} \\n {Y} \\n\")\n","\n","W, B = conv.get_weights()\n","\n","## manual\n","image = image.numpy().squeeze()\n","print(image.shape)\n","print(W.shape, B.shape) ## height, width ,channel, kernel\n","\n","Y_man = np.zeros(shape=(height - k_size + 1, width - k_size + 1, n_filter))\n","for c in range(n_filter):\n","    c_W = W[:, :, :, c] ## 3 by 3\n","    c_b = B[c] ## 1\n","\n","    for h in range(height - n_filter + 1):\n","        for w in range(width - n_filter + 1):\n","            window = image[h:h+k_size, w:w+k_size, :]\n","            conv = np.sum(window * c_W) + c_b\n","\n","            Y_man[h, w, c] = conv\n","\n","Y_man = np.transpose(Y_man, (2, 0, 1))\n","print(f\"Y manual : {Y_man.shape}\")    "],"metadata":{"id":"8LoDpCdUnD_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Conv2D\n","\n","n_neurons = [10, 20, 30] ## num kernels\n","\n","class TestModel(Model):\n","    def __init__(self):\n","        super(TestModel, self).__init__()\n","        global n_neurons\n","\n","        self.conv1 = Conv2D(filters=n_neurons[0], kernel_size=3, activation=\"relu\")\n","        self.conv2 = Conv2D(filters=n_neurons[1], kernel_size=3, activation=\"relu\")\n","        self.conv3 = Conv2D(filters=n_neurons[2], kernel_size=3, activation=\"relu\")\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","\n","        return x\n","\n","model = TestModel()\n","x = tf.random.normal(shape=(32, 28, 28, 3))\n","predictions = model(x)"],"metadata":{"id":"vF0yWlsWnMtb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8.Pooling layer\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1N79DcioU7sm_zmOs4VF0Fxn5qoaZYJuA\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1yPfyaG9SDJOrFsubCIOb84tQYJshKjq7\">\n","<img src=\"https://drive.google.com/uc?export=download&id=1eR10dcmFhHOWxHKy55II4B3YzOwXMVOz\">"],"metadata":{"id":"ql0DGwCdnWDH"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import MaxPooling2D\n","\n","batch_size, height, width, channel = 1, 5, 5, 1\n","pool_size, stride_size = 2, 1\n","\n","x = tf.random.normal(shape=(batch_size, height, width, channel))\n","max_pool = MaxPooling2D(pool_size=pool_size, strides=stride_size)\n","max_pool_result = max_pool(x)\n","\n","print(f\"x : {x.shape} \\n {x.numpy().squeeze()}\")\n","print(f\"max pool : {max_pool_result.shape} \\n {max_pool_result.numpy().squeeze()}\")\n","\n","x = x.numpy().squeeze()\n","max_pool_man = np.zeros(shape=(height - pool_size + 1, width - pool_size + 1))\n","\n","for i in range(height - pool_size + 1):\n","    for j in range(width - pool_size + 1):\n","        window = x[i : i + pool_size, j : j + pool_size]\n","        max_pool_man[i, j] = np.max(window)\n","\n","print(f\"max pool man : {max_pool_man.shape} \\n {max_pool_man}\")"],"metadata":{"id":"c8tAwIpDnx2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import MaxPooling2D\n","\n","batch_size, height, width, channel = 1, 5, 5, 3\n","pool_size, stride_size = 2, 2\n","\n","x = tf.random.normal(shape=(batch_size, height, width, channel))\n","print(f\"x : {x.shape} \\n {np.transpose(x.numpy().squeeze(), (2, 0, 1))} \\n\")\n","\n","max_pool = MaxPooling2D(pool_size=pool_size, strides=stride_size)\n","max_pool_res = max_pool(x)\n","max_pool_res_t = np.transpose(max_pool_res.numpy().squeeze(), (2, 0, 1))\n","print(f\"max pool res : {max_pool_res.shape} \\n {max_pool_res_t} \\n\")\n","\n","## manual\n","x = x.numpy().squeeze()\n","output_h = math.floor((height - pool_size) / stride_size + 1)\n","output_w = math.floor((width - pool_size) / stride_size + 1)\n","\n","max_pool_man = np.zeros(shape=(output_h, output_w, channel))\n","print(max_pool_man.shape)\n","\n","for c in range(channel):\n","    channel_wise_image = x[:, :, c]\n","\n","    output_h_idx = 0\n","    for i in range(0, height - pool_size + 1 , stride_size):\n","        output_w_idx = 0\n","        for j in range(0, width - pool_size + 1, stride_size):\n","            window = channel_wise_image[i : i + pool_size, j : j + pool_size]\n","            max_pool_man[output_h_idx, output_w_idx, c] = np.max(window)\n","\n","            output_w_idx += 1\n","        output_h_idx += 1\n","\n","max_pool_man_t = np.transpose(max_pool_man, (2, 0, 1))\n","print(f\"max pool man : {max_pool_man.shape} \\n {max_pool_man_t} \\n\")"],"metadata":{"id":"umvAYPEZoB-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import AveragePooling2D\n","\n","batch_size, height, width, channel = 1, 5, 5, 1\n","pool_size, stride_size = 2, 1\n","\n","x = tf.random.normal(shape=(batch_size, height, width, channel))\n","avg_pool = AveragePooling2D(pool_size=pool_size, strides=stride_size)\n","avg_pool_result = avg_pool(x)\n","\n","print(f\"x : {x.shape} \\n {x.numpy().squeeze()}\")\n","print(f\"avg pool : {avg_pool_result.shape} \\n {avg_pool_result.numpy().squeeze()}\")\n","\n","x = x.numpy().squeeze()\n","avg_pool_man = np.zeros(shape=(height - pool_size + 1, width - pool_size + 1))\n","\n","for i in range(height - pool_size + 1):\n","    for j in range(width - pool_size + 1):\n","        window = x[i : i + pool_size, j : j + pool_size]\n","        avg_pool_man[i, j] = np.mean(window)\n","\n","print(f\"avg pool man : {avg_pool_man.shape} \\n {avg_pool_man}\")"],"metadata":{"id":"0hCac4Vkn4jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Padding & Stride\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1ybCQoMnO5p491sCX2DIt91gQshUNVVHg\">\n","padding은 input matrix의 상하좌우에 임의의 값을 채워 넣는다.  \n","임의의 값이 0인 경우 Zero Padding이라 부른다. padding을 활용하게 되면, input matrix의 height, width가 축소되는 것을 방지할 수 있다.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1WuVTuFLhmBv-OwHrm0ODQnLQ2cdsxXa_\">\n","\n","stride는 window가 input matrix를 순회할때의 step을 뜻한다.  \n","1칸씩 이동하며 훑어보면 stride = 1이고, 2칸씩 이동(건너뜀)하며 보면 stride=2. \n","따라서 stride를 적용하는 경우 input height, width가 작아질 수 있다.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1A7biAFCT6RSC3MfnxRJHiixHXMAYIc3P\">"],"metadata":{"id":"ESfNKNzQoKg0"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import ZeroPadding2D\n","\n","images = tf.random.normal(shape=(1, 3, 3, 3))\n","print(f\"{images.shape}, \\n {np.transpose(images.numpy().squeeze(), (2, 0, 1))} \\n\")\n","\n","zero_padding = ZeroPadding2D(padding=1)\n","y = zero_padding(images)\n","\n","print(f\"{y.shape}, \\n {np.transpose(y.numpy().squeeze(), (2, 0, 1))} \\n\")"],"metadata":{"id":"VmKgc3H9ofyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","images = tf.random.normal(shape=(1, 28, 28, 3))\n","conv = Conv2D(filters=1, kernel_size=3, padding=\"same\")\n","y = conv(images)\n","\n","print(y.shape)"],"metadata":{"id":"ET9MV99JonYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D\n","\n","images = tf.random.normal(shape=(1, 28, 28, 3))\n","conv = Conv2D(filters=1, kernel_size=3, padding=\"valid\", strides=2)\n","y = conv(images)\n","\n","print(y.shape)"],"metadata":{"id":"79iq6wc2oqhg"},"execution_count":null,"outputs":[]}]}