{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/DL/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from torchtext import transforms\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "DATA_DIR = \"/home/pervinco/Datasets\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 256\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "NUM_WORKERS = min([os.cpu_count(), BATCH_SIZE if BATCH_SIZE > 1 else 0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"{path} folder maded\")\n",
    "    else:\n",
    "        print(f\"{path} is already exist.\")\n",
    "\n",
    "def load_pickle(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_pickle(data, fname):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def make_cache(data_path):\n",
    "    cache_path = f\"{data_path}/cache\"\n",
    "    make_dir(cache_path)\n",
    "\n",
    "    if not os.path.exists(f\"{cache_path}/train.pkl\"):\n",
    "        for name in [\"train\", \"val\", \"test\"]:\n",
    "            pkl_file_name = f\"{cache_path}/{name}.pkl\"\n",
    "\n",
    "            with open(f\"{data_path}/{name}.en\", \"r\") as file:\n",
    "                en = [text.rstrip() for text in file]\n",
    "            \n",
    "            with open(f\"{data_path}/{name}.de\", \"r\") as file:\n",
    "                de = [text.rstrip() for text in file]\n",
    "            \n",
    "            data = [(en_text, de_text) for en_text, de_text in zip(en, de)]\n",
    "            save_pickle(data, pkl_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 22:01:39.997197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 22:01:40.065809: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-23 22:01:40.342010: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-05-23 22:01:40.342044: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-05-23 22:01:40.342047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-05-23 22:01:40.640925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-23 22:01:40.641431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-23 22:01:40.641502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def yield_tokens(data_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30kDataset:\n",
    "    UNK, UNK_IDX = \"<unk>\", 0\n",
    "    PAD, PAD_IDX = \"<pad>\", 1\n",
    "    SOS, SOS_IDX = \"<sos>\", 2\n",
    "    EOS, EOS_IDX = \"<eos>\", 3\n",
    "    SPECIALS = {UNK : UNK_IDX, PAD : PAD_IDX, SOS : SOS_IDX, EOS : EOS_IDX}\n",
    "\n",
    "    URL = \"https://github.com/multi30k/dataset/raw/master/data/task1/raw\"\n",
    "    FILES = [\"test_2016_flickr.de.gz\",\n",
    "             \"test_2016_flickr.en.gz\",\n",
    "             \"train.de.gz\",\n",
    "             \"train.en.gz\",\n",
    "             \"val.de.gz\",\n",
    "             \"val.en.gz\"]\n",
    "    \n",
    "\n",
    "    def __init__(self, data_dir, source_language=\"en\", target_language=\"de\", max_seq_len=256, vocab_min_freq=2):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_min_freq = vocab_min_freq\n",
    "        self.source_language = source_language\n",
    "        self.target_language = target_language\n",
    "\n",
    "        ## 데이터 파일 로드.\n",
    "        self.train = load_pickle(f\"{data_dir}/cache/train.pkl\")\n",
    "        self.valid = load_pickle(f\"{data_dir}/cache/val.pkl\")\n",
    "        self.test = load_pickle(f\"{data_dir}/cache/test.pkl\")\n",
    "\n",
    "        ## tokenizer 정의.\n",
    "        if self.source_language == \"en\":\n",
    "            self.source_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "            self.target_tokenizer = get_tokenizer(\"spacy\", \"de_core_news_sm\")\n",
    "        else:\n",
    "            self.source_tokenizer = get_tokenizer(\"spacy\", \"de_core_news_sm\")\n",
    "            self.target_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "\n",
    "        self.src_vocab, self.trg_vocab = self.get_vocab(self.train)\n",
    "        self.src_transform = self.get_transform(self.src_vocab)\n",
    "        self.trg_transform = self.get_transform(self.trg_vocab)\n",
    "\n",
    "\n",
    "    def yield_tokens(self, train_dataset, is_src):\n",
    "        for text_pair in train_dataset:\n",
    "            if is_src:\n",
    "                yield [str(token) for token in self.source_tokenizer(text_pair[0])]\n",
    "            else:\n",
    "                yield [str(token) for token in self.target_tokenizer(text_pair[1])]\n",
    "\n",
    "\n",
    "    def get_vocab(self, train_dataset):\n",
    "        src_vocab_pickle = f\"{self.data_dir}/cache/vocab_{self.source_language}.pkl\"\n",
    "        trg_vocab_pickle = f\"{self.data_dir}/cache/vocab_{self.target_language}.pkl\"\n",
    "\n",
    "        if os.path.exists(src_vocab_pickle) and os.path.exists(trg_vocab_pickle):\n",
    "            src_vocab = load_pickle(src_vocab_pickle)\n",
    "            trg_vocab = load_pickle(trg_vocab_pickle)\n",
    "        else:\n",
    "            src_vocab = build_vocab_from_iterator(self.yield_tokens(train_dataset, True), min_freq=self.vocab_min_freq, specials=self.SPECIALS.keys())\n",
    "            src_vocab.set_default_index(self.UNK_IDX)\n",
    "\n",
    "            trg_vocab = build_vocab_from_iterator(self.yield_tokens(train_dataset, False), min_freq=self.vocab_min_freq, specials=self.SPECIALS.keys())\n",
    "            trg_vocab.set_default_index(self.UNK_IDX)\n",
    "            \n",
    "        return src_vocab, trg_vocab\n",
    "    \n",
    "\n",
    "    def get_transform(self, vocab):\n",
    "        return transforms.Sequential(transforms.VocabTransform(vocab),\n",
    "                                     transforms.Truncate(self.max_seq_len-2),\n",
    "                                     transforms.AddToken(token=self.SOS_IDX, begin=True),\n",
    "                                     transforms.AddToken(token=self.EOS_IDX, begin=False),\n",
    "                                     transforms.ToTensor(padding_value=self.PAD_IDX))\n",
    "\n",
    "\n",
    "    def collate_fn(self, pairs):\n",
    "        src = [self.source_tokenizer(pair[0]) for pair in pairs]\n",
    "        trg = [self.target_tokenizer(pair[1]) for pair in pairs]\n",
    "        batch_src = self.src_transform(src)\n",
    "        batch_trg = self.trg_transform(trg)\n",
    "\n",
    "        return (batch_src, batch_trg)\n",
    "    \n",
    "\n",
    "    def get_iter(self, batch_size, num_workers):\n",
    "        train_iter = DataLoader(self.train, collate_fn=self.collate_fn, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        valid_iter = DataLoader(self.valid, collate_fn=self.collate_fn, batch_size=batch_size, num_workers=num_workers)\n",
    "        test_iter = DataLoader(self.test, collate_fn=self.collate_fn, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        return train_iter, valid_iter, test_iter\n",
    "    \n",
    "    \n",
    "    def translate(self, model, src_sentence: str, decode_func):\n",
    "        model.eval()\n",
    "        src = self.src_transform([self.source_tokenizer(src_sentence)]).view(1, -1)\n",
    "        num_tokens = src.shape[1]\n",
    "        trg_tokens = decode_func(model, src, max_len=num_tokens + 5, start_symbol=self.SOS_IDX, end_symbol=self.EOS_IDX).flatten().cpu().numpy()\n",
    "        trg_sentence = \" \".join(self.trg_vocab.lookup_tokens(trg_tokens))\n",
    "\n",
    "        return trg_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pervinco/Datasets/Multi30k/cache is already exist.\n"
     ]
    }
   ],
   "source": [
    "make_cache(f\"{DATA_DIR}/Multi30k\")\n",
    "DATASET = Multi30kDataset(data_dir=f\"{DATA_DIR}/Multi30k\", source_language=SRC_LANGUAGE,  target_language=TGT_LANGUAGE,  max_seq_len=MAX_SEQ_LEN, vocab_min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = DATASET.get_iter(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 37])\n",
      "torch.Size([32, 31])\n",
      "[   2   48   12    7    8   32   24 4515   18  125    8  706   14   28\n",
      "  299    5    3    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2   6  71  21 213  11   4 280  91  24 122  41   4 633  88   5   3   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[   2    6   25   58   14  199 1325  318   54    4   61    0   13  131\n",
      "    5    3    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   82   17   55   20 1450    9  620   20    4 1346    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   19  426   11    4   12   38    4 1485  134   11  353  470    5\n",
      "    3    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2   6  35   7   4  30  24   9   4 431   5   3   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[   2    6   12  137  335   14    4 1467   11    4 1365    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   49  167   12   15   97  143    9  425   10  331 2887 2495    5\n",
      "    3    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   12    7    4  221   69   11  146   10  795   70    4  637\n",
      "    9    4   98  496   14  611 1419    5    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   19  112    7  197 3321  945    7    8  101    5    3    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   48   12    0    4 1403   29   86   12 2089  123    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   32  139  133 3267   78    9    4  176  306    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   12   10  583   27  456   14    4 2073 2753    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    0    0   67  166   59    4  334 4163  233  153    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2   6  12 238   4 157 230  28  91 110  56   5   3   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[   2  581    7    4 1500  953    7   28  967  163   55   20    8  116\n",
      "    5    3    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2   6  12  10 246   9   4 694 349  14 101   7   8 100   5   3   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[  2  48  33 185   0  10  21   4  32  84   5   3   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[   2  283   17  129   47    4  638 2387    5    3    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2  245 1032  217   25  426 1890    9    8   40    5    3    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   12   10    9    4 1129  628   20    8 3011    5    3    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2  19  75  73  17 132  11  52  10 624  29  45 119  32   5   3   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n",
      "[   2  618   10   36   77    9    8  235 1020    5    3    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6  367  688   72   18   87 2889 4688    7    8  105    5    3\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2  272 4008   10    7   86    0   15  393 3004 2469    3    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   12   10   36    7    4   89   14    4   51 1188    7  153\n",
      "    5    3    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   34   10   81   62    4   89   14  222 1084  360   77    5\n",
      "    3    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2   57 2252   17    9  111   13    4  487  714    8  223    5    3\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2    6   12   76    7    4  661   24   11   30   11   23  148 4610\n",
      " 5487   27   63  349   14  222  779 3275   78    8 2793   13   28  223\n",
      "  334   15 1434   78    7    8 2093    5    3]\n",
      "[   2   48   12   10  837   14    4   32 1421    5    3    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[   2 1276 1877   21   23 1198   11   23 2758   17  169    4 1352   54\n",
      "   28  599    5    3    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1]\n",
      "[  2 479  35 151   4  60  33   9  27 183   5   3   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1]\n"
     ]
    }
   ],
   "source": [
    "sample_src, sample_tgt = None, None\n",
    "for src, trg in train_iter:\n",
    "    print(src.shape)\n",
    "    print(trg.shape)\n",
    "    \n",
    "    for s in src.numpy():\n",
    "        print(s)\n",
    "\n",
    "    sample_src = src\n",
    "    sample_tgt = trg\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
