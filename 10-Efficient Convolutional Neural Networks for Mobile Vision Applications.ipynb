{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.Depth-Wise Convolution](#1)\n",
    "  - [1-1.Standard Convolution](#1-1)\n",
    "  - [1-2.Depthwise Convoluton](#1-2)\n",
    "\n",
    "[2.Point-Wise Convolution](#2)\n",
    "\n",
    "[3.Depth-Wise Separable Convolution](#3)\n",
    "\n",
    "[4.MobileNet](#4)\n",
    "\n",
    "[5.Training](#5)\n",
    " - [5-1.Hyper Parameters](#5-1)\n",
    " - [5-2.5-2.Load Dataset](#5-2)\n",
    " - [5-3.Train & Valid functions](#5-3)\n",
    " - [5-4.Training iteration](#5-4)\n",
    "\n",
    "[6.Conv MobileNet](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1.Depth Wise Convoltion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 Convolution은 입력된 이미지의 채널과 같은 크기의 채널로 구성된 필터를 적용한다.\n",
    "\n",
    "반면, Depthwise convolution은 입력 이미지의 채널마다 하나의 필터가 적용된다.\n",
    "\n",
    "따라서 더 작은 파라미터 수를 가지며, 입력의 height, width만 조절할 뿐, channel의 값은 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "### 1-1.Standard Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "standard_conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "standard_output = standard_conv(dummy_input)\n",
    "print(standard_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in standard_conv.parameters()) ## 각 파라미터 텐서 p에 있는 원소의 총 개수\n",
    "\n",
    "print(f\"Output shape: {standard_output.shape}\")\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_complexity = (\n",
    "    2 * \n",
    "    standard_output.shape[1] * ## num filters\n",
    "    standard_output.shape[2] * ## height\n",
    "    standard_output.shape[3] * ## width\n",
    "    standard_conv.kernel_size[0] * ## filter size\n",
    "    standard_conv.kernel_size[1] * ## filter size\n",
    "    standard_conv.in_channels ## input channels\n",
    ")\n",
    "\n",
    "print(f\"computation_complexity: {computation_complexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-2\"></a>\n",
    "### 1-2.Depthwise Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_wise_conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, groups=3)\n",
    "depth_wise_output = depth_wise_conv(dummy_input)\n",
    "print(depth_wise_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in depth_wise_conv.parameters()) ## 각 파라미터 텐서 p에 있는 원소의 총 개수\n",
    "\n",
    "print(f\"Output shape: {depth_wise_output.shape}\")\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_height, output_width = depth_wise_output.shape[2], depth_wise_output.shape[3]\n",
    "computation_complexity = 3 * 3 * 3 * output_height * output_width\n",
    "print(f\"Computation complexity: {computation_complexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2.Pointwise Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 내용에서도 몇 번 사용했던, $1\\times 1$ convolution으로, 입력되는 채널의 값이 depthwise convolution의 out_channels와 같기 때문에, 해상도의 크기는 조정되지 않는다.\n",
    "\n",
    "단, pointwise convolution의 out_channels는 설정하는 값에 따라 변화된다.(채널의 크기를 줄이거나 증가시키거나 둘 중 하나.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_wise_conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, groups=3)\n",
    "depth_wise_output = depth_wise_conv(dummy_input)\n",
    "print(depth_wise_output.shape)\n",
    "\n",
    "point_wise_conv = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1)\n",
    "point_wise_output = point_wise_conv(depth_wise_output)\n",
    "print(point_wise_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in point_wise_conv.parameters()) ## 각 파라미터 텐서 p에 있는 원소의 총 개수\n",
    "\n",
    "print(f\"Output shape: {point_wise_output.shape}\")\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3.Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, width_multiplier=1.0):\n",
    "        super().__init__()\n",
    "        in_channels = int(in_channels * width_multiplier)\n",
    "        out_channels = int(out_channels * width_multiplier)\n",
    "\n",
    "        self.depth_wise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        self.point_wise_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth_wise_conv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.point_wise_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "depth_wise_sep_conv = DepthwiseSeparableConv2d(in_channels=3, out_channels=64)\n",
    "output = depth_wise_sep_conv(dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4.MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_multiplier=1.0, init_weights=False):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            DepthwiseSeparableConv2d(in_channels=32, out_channels=64, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=64, out_channels=128, stride=2, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=128, out_channels=128, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=128, out_channels=256, stride=2, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=256, out_channels=256, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=256, out_channels=512, stride=2, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=512, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=512, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=512, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=512, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=512, stride=1, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=512, out_channels=1024, stride=2, width_multiplier=width_multiplier),\n",
    "            DepthwiseSeparableConv2d(in_channels=1024, out_channels=1024, stride=1, width_multiplier=width_multiplier),\n",
    "        )\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5.Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-1\"></a>\n",
    "### 5-1.Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/mammals-classification/data\"\n",
    "save_dir = \"./runs/mammal-classification/resnet\"\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "lr_patience = 10\n",
    "lr_decay_factor = 0.1\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-2\"></a>\n",
    "### 5-2.Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(os.listdir(data_dir))\n",
    "print(classes)\n",
    "\n",
    "images, labels = [], []\n",
    "for str_label in classes:\n",
    "    img_files = os.listdir(f\"{data_dir}/{str_label}\")\n",
    "    for file in img_files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            images.append(f\"{data_dir}/{str_label}/{file}\")\n",
    "            labels.append(str_label)\n",
    "\n",
    "print(f\"Num of Classes : {len(classes)}\")\n",
    "print(f\"Num of files & labels : {len(images)}, {len(labels)}\")\n",
    "print(images[0])\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(images, labels, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(valid_x), len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammalDataset(Dataset):\n",
    "    def __init__(self, images, labels, classes, mean=None, std=None, augmentation=False):\n",
    "        self.classes = classes\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "        if mean is None:\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "\n",
    "        if std is None:\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "\n",
    "        if augmentation:\n",
    "            self.augmentation = A.Compose([\n",
    "                A.OneOf([\n",
    "                    A.Resize(224, 224, p=0.5),\n",
    "                    A.SmallestMaxSize(max_size=[256, 512], p=0.5)\n",
    "                ], p=1),\n",
    "                A.RandomCrop(width=224, height=224),\n",
    "\n",
    "                A.HorizontalFlip(p=0.4),\n",
    "                A.RGBShift(p=0.3),\n",
    "\n",
    "                A.ShiftScaleRotate(p=1.0, shift_limit_x=(-0.06, 0.06), shift_limit_y=(-0.06, 0.06), \n",
    "                                   scale_limit=(-0.099, 0.100), \n",
    "                                   rotate_limit=(-180, 180), \n",
    "                                   interpolation=0, \n",
    "                                   border_mode=0, \n",
    "                                   value=(0, 0, 0)),\n",
    "                A.RandomBrightnessContrast(p=1.0, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "                A.AdvancedBlur(p=0.35, blur_limit=(3, 7), \n",
    "                               sigmaX_limit=(0.2, 1.0), \n",
    "                               sigmaY_limit=(0.2, 1.0), \n",
    "                               rotate_limit=(-90, 90), \n",
    "                               beta_limit=(0.5, 8.0), \n",
    "                               noise_limit=(0.9, 1.1)),\n",
    "\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = A.Compose([\n",
    "                A.Resize(224, 224, p=1),\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.classes.index(label)\n",
    "\n",
    "        image = self.augmentation(image=image)[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MammalDataset(train_x, train_y, classes, augmentation=True)\n",
    "valid_dataset = MammalDataset(valid_x, valid_y, classes, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloder = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-3\"></a>\n",
    "### 5-3.Train & Valid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_cost, train_acc = 0.0, 0.0\n",
    "    for x, y in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        cost = criterion(y_pred, y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_cost += cost.item() * x.size(0)\n",
    "\n",
    "        _, pred_labels = torch.max(y_pred, 1)\n",
    "        train_acc += (pred_labels == y).sum().item()\n",
    "\n",
    "    train_cost /= len(dataloader.dataset)\n",
    "    train_acc /= len(dataloader.dataset)\n",
    "\n",
    "    return train_cost, train_acc\n",
    "\n",
    "\n",
    "def valid(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    valid_cost, valid_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dataloader, desc=\"Valid\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            cost = criterion(y_pred, y)\n",
    "\n",
    "            valid_cost += cost.item() * x.size(0)\n",
    "\n",
    "            _, pred_labels = torch.max(y_pred, 1)\n",
    "            valid_acc += (pred_labels == y).sum().item()\n",
    "\n",
    "        valid_cost /= len(dataloader.dataset)\n",
    "        valid_acc /= len(dataloader.dataset)\n",
    "\n",
    "    return valid_cost, valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-4\"></a>\n",
    "### 5-4.Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(num_classes=len(classes), width_multiplier=1.0, init_weights=True)\n",
    "summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "train_costs, train_accs = [], []\n",
    "valid_costs, valid_accs = [], []\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 10\n",
    "best_valid_cost = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_cost, train_acc = train(model, train_dataloder, criterion, optimizer, device)\n",
    "    valid_cost, valid_acc = valid(model, valid_dataloader, criterion, device)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs:\n",
    "        print(f\"Epoch : {epoch} | {epochs}\")\n",
    "        print(f\"\\tTrain Loss : {train_cost:.4f}, Train Acc : {train_acc:.4f}\")\n",
    "        print(f\"\\tValid Loss : {valid_cost:.4f}, Valid Acc : {valid_acc:.4f}\")\n",
    "\n",
    "    train_costs.append(train_cost)\n",
    "    valid_costs.append(valid_cost)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    ## Early Stopping\n",
    "    if valid_cost < best_valid_cost:\n",
    "        best_valid_cost = valid_cost\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after epoch {epoch} due to no improvement in validation loss\")\n",
    "        break  # Break out of the loop to stop training\n",
    "\n",
    "    scheduler.step(valid_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_costs, label='Training cost')\n",
    "plt.plot(valid_costs, label='Validation cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.title('MobileNet Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accs, label='Training accs')\n",
    "plt.plot(valid_accs, label='Validation accs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('MobileNet Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6.Conv MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_multiplier=1.0, init_weights=False):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBNReLU(in_channels=32, out_channels=64, stride=1),\n",
    "            ConvBNReLU(in_channels=64, out_channels=128, stride=2),\n",
    "            ConvBNReLU(in_channels=128, out_channels=128, stride=1),\n",
    "            ConvBNReLU(in_channels=128, out_channels=256, stride=2),\n",
    "            ConvBNReLU(in_channels=256, out_channels=256, stride=1),\n",
    "            ConvBNReLU(in_channels=256, out_channels=512, stride=2),\n",
    "            ConvBNReLU(in_channels=512, out_channels=512, stride=1),\n",
    "            ConvBNReLU(in_channels=512, out_channels=512, stride=1),\n",
    "            ConvBNReLU(in_channels=512, out_channels=512, stride=1),\n",
    "            ConvBNReLU(in_channels=512, out_channels=512, stride=1),\n",
    "            ConvBNReLU(in_channels=512, out_channels=512, stride=1),\n",
    "            ConvBNReLU(in_channels=512, out_channels=1024, stride=2),\n",
    "            ConvBNReLU(in_channels=1024, out_channels=1024, stride=1),\n",
    "        )\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMobileNet(num_classes=len(classes), init_weights=True)\n",
    "summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "train_costs, train_accs = [], []\n",
    "valid_costs, valid_accs = [], []\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 10\n",
    "best_valid_cost = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_cost, train_acc = train(model, train_dataloder, criterion, optimizer, device)\n",
    "    valid_cost, valid_acc = valid(model, valid_dataloader, criterion, device)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs:\n",
    "        print(f\"Epoch : {epoch} | {epochs}\")\n",
    "        print(f\"\\tTrain Loss : {train_cost:.4f}, Train Acc : {train_acc:.4f}\")\n",
    "        print(f\"\\tValid Loss : {valid_cost:.4f}, Valid Acc : {valid_acc:.4f}\")\n",
    "\n",
    "    train_costs.append(train_cost)\n",
    "    valid_costs.append(valid_cost)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    ## Early Stopping\n",
    "    if valid_cost < best_valid_cost:\n",
    "        best_valid_cost = valid_cost\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after epoch {epoch} due to no improvement in validation loss\")\n",
    "        break  # Break out of the loop to stop training\n",
    "\n",
    "    scheduler.step(valid_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_costs, label='Training cost')\n",
    "plt.plot(valid_costs, label='Validation cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.title('ConvMobileNet Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accs, label='Training accs')\n",
    "plt.plot(valid_accs, label='Validation accs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('ConvMobileNet Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
