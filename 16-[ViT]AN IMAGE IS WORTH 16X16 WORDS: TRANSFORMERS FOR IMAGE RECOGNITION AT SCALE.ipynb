{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-[ViT]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ml_collections\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.modules.utils import _pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super().__init__()\n",
    "        img_size = _pair(img_size)  # img_size : (224, 224)\n",
    "        patch_size = _pair(config.patches[\"size\"])  # patch_size : (16, 16)\n",
    "        \n",
    "        # 전체 패치 수 계산: (224/16) * (224/16) = 196\n",
    "        n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "\n",
    "        # Conv2d 출력 : [batch_size, embedd_dim, 14, 14]\n",
    "        # stride를 patch_size로 설정함으로써 n_patches개의 patch가 생성된다.\n",
    "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels, out_channels=config.hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # position_embeddings : [1, 197, embedd_dim]\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        \n",
    "        # cls_token : [1, 1, embedd_dim], 클래스 분류를 위한 클래스 토큰.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]  # x : [batch_size, 3, 224, 224]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # expanded cls_tokens : [batch_size, 1, embedd_dim]\n",
    "        \n",
    "        x = self.patch_embeddings(x) # image -> patches : [batch_size, embedd_dim, 14, 14], 여기서 14는 224 // 16\n",
    "        x = x.flatten(2)  # flatten : [batch_size, embedd_dim, 196]\n",
    "        x = x.transpose(-1, -2)  # transpose : [batch_size, 196, embedd_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # cat : [batch_size, 197, embedd_dim]\n",
    "\n",
    "        embeddings = x + self.position_embeddings  # broad casting을 통한 position_embeddings 추가 : [batch_size, 197, embedd_dim]\n",
    "        embeddings = self.dropout(embeddings)  # dropout : [batch_size, 197, embedd_dim]\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
