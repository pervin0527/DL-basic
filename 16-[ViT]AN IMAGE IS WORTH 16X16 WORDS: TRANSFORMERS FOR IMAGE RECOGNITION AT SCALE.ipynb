{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-[ViT]AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from datasets import load_dataset\n",
    "from os.path import join as pjoin\n",
    "from models.resnet import ResNetV2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.lr_schedulers import WarmupConstantSchedule, WarmupLinearSchedule, WarmupCosineSchedule\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \n",
    "          \"relu\": torch.nn.functional.relu, \n",
    "          \"swish\": torch.nn.functional.silu}\n",
    "\n",
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params/1000000\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1.Generate Pathes and Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)  # img_size : (224, 224)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None: ## grid는 conv의 feature map을 몇 개의 부분으로 분할할지 나타낸다.\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        \n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])  # patch_size : (16, 16)\n",
    "            # 전체 패치 수 계산: (224/16) * (224/16) = 196\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "\n",
    "        # Conv2d 출력 : [batch_size, embedd_dim, 14, 14]\n",
    "        # stride를 patch_size로 설정함으로써 n_patches개의 patch가 생성된다.\n",
    "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels, out_channels=config.hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # position_embeddings : [1, 197, embedd_dim]\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        \n",
    "        # cls_token : [1, 1, embedd_dim], 클래스 분류를 위한 클래스 토큰.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]  # x : [batch_size, 3, 224, 224]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # expanded cls_tokens : [batch_size, 1, embedd_dim]\n",
    "        \n",
    "        x = self.patch_embeddings(x) # image -> patches : [batch_size, embedd_dim, 14, 14], 여기서 14는 224 // 16\n",
    "        x = x.flatten(2)  # flatten : [batch_size, embedd_dim, 196]\n",
    "        x = x.transpose(-1, -2)  # transpose : [batch_size, 196, embedd_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # cat : [batch_size, 197, embedd_dim]\n",
    "\n",
    "        embeddings = x + self.position_embeddings  # broad casting을 통한 position_embeddings 추가 : [batch_size, 197, embedd_dim]\n",
    "        embeddings = self.dropout(embeddings)  # dropout : [batch_size, 197, embedd_dim]\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2.Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = nn.Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = nn.Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3.Multi Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super().__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = nn.Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "\n",
    "        return attention_output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4.Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2th(weights, conv=False):\n",
    "    \"\"\"\n",
    "    [height, width, input_channel, output_channel] -> [output_channel, input_channel, height, width]\n",
    "    \"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    \n",
    "    return torch.from_numpy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, visualize):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = MLP(config)\n",
    "        self.attn = Attention(config, visualize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "        \n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "\n",
    "        return x, weights\n",
    "    \n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super().__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "\n",
    "        return encoded, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "\n",
    "        return encoded, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6.ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=1000, zero_head=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis=False)\n",
    "        self.head = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x, attn_weights = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "            return loss\n",
    "        \n",
    "        else:\n",
    "            return logits, attn_weights\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.weight)\n",
    "                nn.init.zeros_(self.head.bias)\n",
    "            else:\n",
    "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
    "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
    "\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10000\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "valid_term = 100\n",
    "\n",
    "learning_rate = 0.003\n",
    "weight_decay = 0\n",
    "decay_type = \"cosine\" # [\"cosine\", \"linear\"]\n",
    "warmup_steps = 500 ## need to fix steps -> epoch\n",
    "max_grad_norm = 1.0\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "data_dir = \"/home/pervinco/Datasets/mammals-classification/data\"\n",
    "save_dir = \"./runs/ViT\"\n",
    "pretrained_weight = \"/home/pervinco/ViT-pytorch/checkpoint/imagenet21k/ViT-B_16.npz\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_model(model):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(save_dir, \"train_checkpoint.bin\")\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammalDataset(Dataset):\n",
    "    def __init__(self, images, labels, classes, mean=None, std=None, augmentation=False):\n",
    "        self.classes = classes\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "        if mean is None:\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "\n",
    "        if std is None:\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "\n",
    "        if augmentation:\n",
    "            self.augmentation = A.Compose([\n",
    "                A.OneOf([\n",
    "                    A.Resize(224, 224, p=0.5),\n",
    "                    A.SmallestMaxSize(max_size=[256, 512], p=0.5)\n",
    "                ], p=1),\n",
    "                A.RandomCrop(width=224, height=224),\n",
    "\n",
    "                A.HorizontalFlip(p=0.4),\n",
    "                A.RGBShift(p=0.3),\n",
    "\n",
    "                A.ShiftScaleRotate(p=1.0, shift_limit_x=(-0.06, 0.06), shift_limit_y=(-0.06, 0.06), \n",
    "                                   scale_limit=(-0.099, 0.100), \n",
    "                                   rotate_limit=(-180, 180), \n",
    "                                   interpolation=0, \n",
    "                                   border_mode=0, \n",
    "                                   value=(0, 0, 0)),\n",
    "                A.RandomBrightnessContrast(p=1.0, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "                A.AdvancedBlur(p=0.35, blur_limit=(3, 7), \n",
    "                               sigmaX_limit=(0.2, 1.0), \n",
    "                               sigmaY_limit=(0.2, 1.0), \n",
    "                               rotate_limit=(-90, 90), \n",
    "                               beta_limit=(0.5, 8.0), \n",
    "                               noise_limit=(0.9, 1.1)),\n",
    "\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = A.Compose([\n",
    "                A.Resize(224, 224, p=1),\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.classes.index(label)\n",
    "\n",
    "        image = self.augmentation(image=image)[\"image\"]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['african_elephant', 'alpaca', 'american_bison', 'anteater', 'arctic_fox', 'armadillo', 'baboon', 'badger', 'blue_whale', 'brown_bear', 'camel', 'dolphin', 'giraffe', 'groundhog', 'highland_cattle', 'horse', 'jackal', 'kangaroo', 'koala', 'manatee', 'mongoose', 'mountain_goat', 'opossum', 'orangutan', 'otter', 'polar_bear', 'porcupine', 'red_panda', 'rhinoceros', 'sea_lion', 'seal', 'snow_leopard', 'squirrel', 'sugar_glider', 'tapir', 'vampire_bat', 'vicuna', 'walrus', 'warthog', 'water_buffalo', 'weasel', 'wildebeest', 'wombat', 'yak', 'zebra']\n",
      "Num of Classes : 45\n",
      "Num of files & labels : 13751, 13751\n",
      "/home/pervinco/Datasets/mammals-classification/data/african_elephant/african_elephant-0125.jpg\n",
      "12375 12375\n",
      "1376 1376\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(os.listdir(data_dir))\n",
    "print(classes)\n",
    "\n",
    "images, labels = [], []\n",
    "for str_label in classes:\n",
    "    img_files = os.listdir(f\"{data_dir}/{str_label}\")\n",
    "    for file in img_files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            images.append(f\"{data_dir}/{str_label}/{file}\")\n",
    "            labels.append(str_label)\n",
    "\n",
    "print(f\"Num of Classes : {len(classes)}\")\n",
    "print(f\"Num of files & labels : {len(images)}, {len(labels)}\")\n",
    "print(images[0])\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(images, labels, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(valid_x), len(valid_y))\n",
    "\n",
    "train_dataset = MammalDataset(train_x, train_y, classes, augmentation=True)\n",
    "valid_dataset = MammalDataset(valid_x, valid_y, classes, augmentation=False)\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.833261\n"
     ]
    }
   ],
   "source": [
    "config = get_b16_config()\n",
    "\n",
    "model = VisionTransformer(config, img_size, zero_head=True, num_classes=len(classes))\n",
    "model.load_from(np.load(pretrained_weight))\n",
    "model.to(device)\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "logger.info(\"{}\".format(config))\n",
    "logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, writer, test_loader, global_step):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", valid_term)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = (all_preds == all_label).mean()\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_steps, batch_size):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=os.path.join(save_dir, \"logs\", \"train\"))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "    t_total = num_steps\n",
    "    if decay_type == \"cosine\":\n",
    "        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "    else:\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Total optimization steps = %d\", num_steps)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",batch_size * gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "\n",
    "    model.zero_grad()\n",
    "    losses = AverageMeter()\n",
    "    global_step, best_acc = 0, 0\n",
    "    while True:\n",
    "        model.train()\n",
    "        epoch_iterator = tqdm(train_dataloder,\n",
    "                              desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                              bar_format=\"{l_bar}{r_bar}\",\n",
    "                              dynamic_ncols=True)\n",
    "        \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x, y = batch\n",
    "            loss = model(x, y)\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                losses.update(loss.item()*gradient_accumulation_steps)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "\n",
    "                epoch_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val))\n",
    "                writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "                writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_last_lr()[0], global_step=global_step)\n",
    "\n",
    "                if global_step % valid_term == 0:\n",
    "                    accuracy = valid(model, writer, valid_dataloader, global_step)\n",
    "                    if best_acc < accuracy:\n",
    "                        save_model(model)\n",
    "                        best_acc = accuracy\n",
    "                    model.train()\n",
    "\n",
    "                if global_step % t_total == 0:\n",
    "                    break\n",
    "        losses.reset()\n",
    "        if global_step % t_total == 0:\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n",
    "    logger.info(\"End Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=3.77810): 100%|| 43/43 [00:05<00:00,  7.69it/s]0:33,  8.60it/s]\n",
      "Validating... (loss=3.67745): 100%|| 43/43 [00:05<00:00,  7.78it/s]00:21,  8.62it/s]\n",
      "Validating... (loss=3.47663): 100%|| 43/43 [00:05<00:00,  7.75it/s]00:10,  8.62it/s]\n",
      "Training (387 / 10000 Steps) (loss=3.18075): 100%|| 387/387 [01:06<00:00,  5.81it/s]\n",
      "Validating... (loss=3.18819): 100%|| 43/43 [00:05<00:00,  7.62it/s]0:53,  6.97it/s]\n",
      "Validating... (loss=2.80209): 100%|| 43/43 [00:05<00:00,  7.70it/s]00:31,  8.60it/s]\n",
      "Validating... (loss=2.35966): 100%|| 43/43 [00:05<00:00,  7.71it/s]00:20,  8.59it/s]\n",
      "Validating... (loss=1.96897): 100%|| 43/43 [00:05<00:00,  7.66it/s]00:08,  8.59it/s]\n",
      "Training (774 / 10000 Steps) (loss=2.02306): 100%|| 387/387 [01:12<00:00,  5.34it/s]\n",
      "Validating... (loss=1.64311): 100%|| 43/43 [00:05<00:00,  7.84it/s]0:42,  8.61it/s]\n",
      "Validating... (loss=1.38045): 100%|| 43/43 [00:05<00:00,  7.82it/s]00:30,  8.58it/s]\n",
      "Validating... (loss=1.16798): 100%|| 43/43 [00:05<00:00,  7.76it/s]<00:18,  8.59it/s]\n",
      "Validating... (loss=1.00171): 100%|| 43/43 [00:05<00:00,  7.94it/s]<00:07,  8.61it/s]\n",
      "Training (1161 / 10000 Steps) (loss=1.17889): 100%|| 387/387 [01:12<00:00,  5.34it/s]\n",
      "Validating... (loss=0.84615): 100%|| 43/43 [00:05<00:00,  7.88it/s]00:40,  8.60it/s]\n",
      "Validating... (loss=0.73882): 100%|| 43/43 [00:05<00:00,  7.74it/s]<00:28,  8.59it/s]\n",
      "Validating... (loss=0.65940): 100%|| 43/43 [00:05<00:00,  7.78it/s]<00:17,  8.59it/s]\n",
      "Validating... (loss=0.57202): 100%|| 43/43 [00:05<00:00,  7.76it/s]<00:05,  8.61it/s]\n",
      "Training (1548 / 10000 Steps) (loss=1.04911): 100%|| 387/387 [01:12<00:00,  5.35it/s]\n",
      "Validating... (loss=0.51147): 100%|| 43/43 [00:05<00:00,  7.69it/s]00:39,  8.61it/s]\n",
      "Validating... (loss=0.44845): 100%|| 43/43 [00:05<00:00,  7.78it/s]<00:27,  8.58it/s]\n",
      "Validating... (loss=0.40492): 100%|| 43/43 [00:05<00:00,  7.66it/s]<00:15,  8.58it/s]\n",
      "Validating... (loss=0.36768): 100%|| 43/43 [00:05<00:00,  7.78it/s]<00:04,  8.62it/s]\n",
      "Training (1935 / 10000 Steps) (loss=0.64994): 100%|| 387/387 [01:12<00:00,  5.35it/s]\n",
      "Validating... (loss=0.33052): 100%|| 43/43 [00:05<00:00,  7.44it/s]00:37,  8.61it/s]\n",
      "Validating... (loss=0.30802): 100%|| 43/43 [00:05<00:00,  7.61it/s]<00:25,  8.60it/s]\n",
      "Validating... (loss=0.28705): 100%|| 43/43 [00:05<00:00,  7.93it/s]<00:14,  8.56it/s]\n",
      "Validating... (loss=0.26980): 100%|| 43/43 [00:05<00:00,  7.74it/s]<00:02,  8.64it/s]\n",
      "Training (2322 / 10000 Steps) (loss=0.78669): 100%|| 387/387 [01:11<00:00,  5.38it/s]\n",
      "Validating... (loss=0.25837): 100%|| 43/43 [00:05<00:00,  7.65it/s]00:36,  8.60it/s]\n",
      "Validating... (loss=0.24049): 100%|| 43/43 [00:05<00:00,  7.81it/s]<00:24,  8.59it/s]\n",
      "Validating... (loss=0.21643): 100%|| 43/43 [00:05<00:00,  7.85it/s]<00:12,  8.57it/s]\n",
      "Validating... (loss=0.19406): 100%|| 43/43 [00:05<00:00,  7.68it/s]<00:01,  8.63it/s]\n",
      "Training (2709 / 10000 Steps) (loss=0.80955): 100%|| 387/387 [01:12<00:00,  5.37it/s]\n",
      "Validating... (loss=0.19083): 100%|| 43/43 [00:05<00:00,  7.89it/s]00:34,  8.57it/s]\n",
      "Validating... (loss=0.18070): 100%|| 43/43 [00:05<00:00,  7.72it/s]<00:22,  8.58it/s]\n",
      "Validating... (loss=0.16493): 100%|| 43/43 [00:05<00:00,  7.56it/s]<00:11,  8.58it/s]\n",
      "Training (3096 / 10000 Steps) (loss=0.64607): 100%|| 387/387 [01:05<00:00,  5.88it/s]\n",
      "Validating... (loss=0.16659): 100%|| 43/43 [00:05<00:00,  8.00it/s]6:06,  1.05it/s]\n",
      "Validating... (loss=0.15895): 100%|| 43/43 [00:05<00:00,  7.74it/s]<00:34,  8.23it/s]\n",
      "Validating... (loss=0.14749): 100%|| 43/43 [00:05<00:00,  7.74it/s]<00:21,  8.60it/s]\n",
      "Validating... (loss=0.13837): 100%|| 43/43 [00:05<00:00,  7.75it/s]<00:09,  8.59it/s]\n",
      "Training (3483 / 10000 Steps) (loss=0.56579): 100%|| 387/387 [01:11<00:00,  5.39it/s]\n",
      "Validating... (loss=0.14171): 100%|| 43/43 [00:05<00:00,  7.65it/s]00:45,  8.18it/s]\n",
      "Validating... (loss=0.14030): 100%|| 43/43 [00:05<00:00,  7.84it/s]<00:31,  8.59it/s]\n",
      "Validating... (loss=0.13897): 100%|| 43/43 [00:05<00:00,  7.84it/s]<00:19,  8.59it/s]\n",
      "Validating... (loss=0.13417): 100%|| 43/43 [00:05<00:00,  8.01it/s]<00:08,  8.58it/s]\n",
      "Training (3870 / 10000 Steps) (loss=0.24494): 100%|| 387/387 [01:11<00:00,  5.39it/s]\n",
      "Validating... (loss=0.12765): 100%|| 43/43 [00:05<00:00,  7.90it/s]00:41,  8.59it/s]\n",
      "Validating... (loss=0.12826): 100%|| 43/43 [00:05<00:00,  7.75it/s]<00:29,  8.60it/s]\n",
      "Validating... (loss=0.11933): 100%|| 43/43 [00:05<00:00,  7.96it/s]<00:18,  8.59it/s]\n",
      "Validating... (loss=0.11692): 100%|| 43/43 [00:05<00:00,  7.88it/s]<00:06,  8.62it/s]\n",
      "Training (4257 / 10000 Steps) (loss=0.33147): 100%|| 387/387 [01:11<00:00,  5.42it/s]\n",
      "Validating... (loss=0.10957): 100%|| 43/43 [00:05<00:00,  7.71it/s]00:40,  8.53it/s]\n",
      "Validating... (loss=0.11473): 100%|| 43/43 [00:05<00:00,  7.72it/s]<00:28,  8.60it/s]\n",
      "Validating... (loss=0.11190): 100%|| 43/43 [00:05<00:00,  7.70it/s]<00:16,  8.58it/s]\n",
      "Validating... (loss=0.10627): 100%|| 43/43 [00:05<00:00,  7.76it/s]<00:05,  8.62it/s]\n",
      "Training (4644 / 10000 Steps) (loss=0.23199): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.10301): 100%|| 43/43 [00:05<00:00,  7.90it/s]00:38,  8.58it/s]\n",
      "Validating... (loss=0.10215): 100%|| 43/43 [00:05<00:00,  7.75it/s]<00:26,  8.59it/s]\n",
      "Validating... (loss=0.10089): 100%|| 43/43 [00:05<00:00,  7.71it/s]<00:15,  8.57it/s]\n",
      "Validating... (loss=0.09876): 100%|| 43/43 [00:05<00:00,  7.84it/s]<00:03,  8.64it/s]\n",
      "Training (5031 / 10000 Steps) (loss=0.20685): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.09830): 100%|| 43/43 [00:05<00:00,  7.70it/s]00:37,  8.59it/s]\n",
      "Validating... (loss=0.10018): 100%|| 43/43 [00:05<00:00,  7.71it/s]<00:25,  8.61it/s]\n",
      "Validating... (loss=0.10012): 100%|| 43/43 [00:05<00:00,  7.89it/s]<00:13,  8.55it/s]\n",
      "Validating... (loss=0.10011): 100%|| 43/43 [00:05<00:00,  7.63it/s]<00:02,  8.63it/s]\n",
      "Training (5418 / 10000 Steps) (loss=0.38561): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.09539): 100%|| 43/43 [00:05<00:00,  7.79it/s]00:35,  8.59it/s]\n",
      "Validating... (loss=0.09431): 100%|| 43/43 [00:05<00:00,  7.80it/s]<00:23,  8.60it/s]\n",
      "Validating... (loss=0.10107): 100%|| 43/43 [00:05<00:00,  7.60it/s]<00:12,  8.58it/s]\n",
      "Validating... (loss=0.09930): 100%|| 43/43 [00:05<00:00,  7.77it/s]<00:00,  8.62it/s]\n",
      "Training (5805 / 10000 Steps) (loss=0.49608): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.09384): 100%|| 43/43 [00:05<00:00,  7.67it/s]00:34,  8.58it/s]\n",
      "Validating... (loss=0.09152): 100%|| 43/43 [00:05<00:00,  7.69it/s]<00:22,  8.60it/s]\n",
      "Validating... (loss=0.09014): 100%|| 43/43 [00:05<00:00,  7.80it/s]<00:10,  8.59it/s]\n",
      "Training (6192 / 10000 Steps) (loss=0.37081): 100%|| 387/387 [01:05<00:00,  5.88it/s]\n",
      "Validating... (loss=0.08343): 100%|| 43/43 [00:05<00:00,  7.95it/s]1:43,  3.68it/s]\n",
      "Validating... (loss=0.08476): 100%|| 43/43 [00:05<00:00,  7.77it/s]<00:32,  8.61it/s]\n",
      "Validating... (loss=0.08064): 100%|| 43/43 [00:05<00:00,  7.89it/s]<00:20,  8.58it/s]\n",
      "Validating... (loss=0.08193): 100%|| 43/43 [00:05<00:00,  7.54it/s]<00:09,  8.53it/s]\n",
      "Training (6579 / 10000 Steps) (loss=0.41686): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.08265): 100%|| 43/43 [00:05<00:00,  7.61it/s]00:43,  8.49it/s]\n",
      "Validating... (loss=0.07844): 100%|| 43/43 [00:05<00:00,  7.83it/s]<00:31,  8.58it/s]\n",
      "Validating... (loss=0.08026): 100%|| 43/43 [00:05<00:00,  7.86it/s]<00:19,  8.58it/s]\n",
      "Validating... (loss=0.08585): 100%|| 43/43 [00:05<00:00,  7.50it/s]<00:07,  8.56it/s]\n",
      "Training (6966 / 10000 Steps) (loss=0.06620): 100%|| 387/387 [01:11<00:00,  5.38it/s]\n",
      "Validating... (loss=0.08840): 100%|| 43/43 [00:05<00:00,  7.79it/s]00:41,  8.46it/s]\n",
      "Validating... (loss=0.08613): 100%|| 43/43 [00:05<00:00,  7.81it/s]<00:29,  8.59it/s]\n",
      "Validating... (loss=0.08915): 100%|| 43/43 [00:05<00:00,  7.77it/s]<00:17,  8.56it/s]\n",
      "Validating... (loss=0.08920): 100%|| 43/43 [00:05<00:00,  7.77it/s]<00:06,  8.62it/s]\n",
      "Training (7353 / 10000 Steps) (loss=0.31658): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.09303): 100%|| 43/43 [00:05<00:00,  7.78it/s]00:39,  8.60it/s]\n",
      "Validating... (loss=0.09343): 100%|| 43/43 [00:05<00:00,  7.59it/s]<00:28,  8.58it/s]\n",
      "Validating... (loss=0.09222): 100%|| 43/43 [00:05<00:00,  7.73it/s]<00:16,  8.57it/s]\n",
      "Validating... (loss=0.09020): 100%|| 43/43 [00:05<00:00,  7.77it/s]<00:04,  8.60it/s]\n",
      "Training (7740 / 10000 Steps) (loss=0.52972): 100%|| 387/387 [01:12<00:00,  5.34it/s]\n",
      "Validating... (loss=0.08815): 100%|| 43/43 [00:05<00:00,  7.77it/s]00:38,  8.59it/s]\n",
      "Validating... (loss=0.08647): 100%|| 43/43 [00:05<00:00,  7.62it/s]<00:26,  8.59it/s]\n",
      "Validating... (loss=0.08728): 100%|| 43/43 [00:05<00:00,  7.73it/s]<00:14,  8.57it/s]\n",
      "Validating... (loss=0.08801): 100%|| 43/43 [00:05<00:00,  7.74it/s]<00:03,  8.61it/s]\n",
      "Training (8127 / 10000 Steps) (loss=0.29959): 100%|| 387/387 [01:11<00:00,  5.39it/s]\n",
      "Validating... (loss=0.08776): 100%|| 43/43 [00:05<00:00,  7.74it/s]00:36,  8.60it/s]\n",
      "Validating... (loss=0.08923): 100%|| 43/43 [00:05<00:00,  7.64it/s]<00:25,  8.57it/s]\n",
      "Validating... (loss=0.08731): 100%|| 43/43 [00:05<00:00,  7.59it/s]<00:13,  8.55it/s]\n",
      "Validating... (loss=0.08555): 100%|| 43/43 [00:05<00:00,  7.65it/s]<00:01,  8.62it/s]\n",
      "Training (8514 / 10000 Steps) (loss=0.10849): 100%|| 387/387 [01:12<00:00,  5.37it/s]\n",
      "Validating... (loss=0.08365): 100%|| 43/43 [00:05<00:00,  7.95it/s]00:35,  8.57it/s]\n",
      "Validating... (loss=0.08298): 100%|| 43/43 [00:05<00:00,  7.79it/s]<00:23,  8.58it/s]\n",
      "Validating... (loss=0.08128): 100%|| 43/43 [00:05<00:00,  7.61it/s]<00:11,  8.58it/s]\n",
      "Validating... (loss=0.08158): 100%|| 43/43 [00:05<00:00,  7.86it/s]<00:00,  8.60it/s]\n",
      "Training (8901 / 10000 Steps) (loss=0.27843): 100%|| 387/387 [01:11<00:00,  5.42it/s]\n",
      "Validating... (loss=0.08100): 100%|| 43/43 [00:05<00:00,  7.77it/s]00:33,  8.59it/s]\n",
      "Validating... (loss=0.08135): 100%|| 43/43 [00:05<00:00,  7.69it/s]<00:22,  8.54it/s]\n",
      "Validating... (loss=0.08153): 100%|| 43/43 [00:05<00:00,  7.57it/s]<00:10,  8.58it/s]\n",
      "Training (9288 / 10000 Steps) (loss=0.12273): 100%|| 387/387 [01:06<00:00,  5.84it/s]\n",
      "Validating... (loss=0.08136): 100%|| 43/43 [00:05<00:00,  7.79it/s]00:57,  6.52it/s]\n",
      "Validating... (loss=0.08126): 100%|| 43/43 [00:05<00:00,  7.85it/s]<00:32,  8.60it/s]\n",
      "Validating... (loss=0.08132): 100%|| 43/43 [00:05<00:00,  7.70it/s]<00:20,  8.59it/s]\n",
      "Validating... (loss=0.08130): 100%|| 43/43 [00:05<00:00,  7.64it/s]<00:08,  8.56it/s]\n",
      "Training (9675 / 10000 Steps) (loss=0.22683): 100%|| 387/387 [01:11<00:00,  5.40it/s]\n",
      "Validating... (loss=0.08139): 100%|| 43/43 [00:05<00:00,  7.56it/s]00:42,  8.57it/s]\n",
      "Validating... (loss=0.08142): 100%|| 43/43 [00:05<00:00,  7.51it/s]<00:30,  8.60it/s]\n",
      "Validating... (loss=0.08144): 100%|| 43/43 [00:05<00:00,  7.65it/s]<00:19,  8.56it/s]\n",
      "Validating... (loss=0.08144): 100%|| 43/43 [00:05<00:00,  7.53it/s]9<00:07,  8.59it/s]\n",
      "Training (10000 / 10000 Steps) (loss=0.11804):  84%|| 324/387 [01:05<00:12,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_with_mAP(model, test_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs if not isinstance(outputs, tuple) else outputs[0]\n",
    "\n",
    "            loss = loss_fct(logits, labels)\n",
    "            eval_losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits_np = np.concatenate(all_logits, axis=0)\n",
    "    all_labels_np = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    print(\"Logits shape after concatenation:\", all_logits_np.shape)\n",
    "    print(\"Labels shape after concatenation:\", all_labels_np.shape)\n",
    "\n",
    "    APs = []\n",
    "    for i in range(num_classes):\n",
    "        true_labels = (all_labels_np == i).astype(int)        \n",
    "        AP = average_precision_score(true_labels, all_logits_np[:, i])\n",
    "        APs.append(AP)\n",
    "\n",
    "    mAP = np.mean(APs)  # 클래스별 AP의 평균 계산\n",
    "    print(\"mAP:\", mAP)\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid mAP: %2.5f\" % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape after concatenation: (1376, 45)\n",
      "Labels shape after concatenation: (1376,)\n",
      "mAP: 0.9877495873779184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9877495873779184"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_with_mAP(model, valid_dataloader, device, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
