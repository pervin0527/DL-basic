{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/AiLab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import dgl\n",
    "import torch\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from math import sqrt\n",
    "from time import time\n",
    "from rdkit import Chem\n",
    "from dgl import function as func\n",
    "from dgl.nn.functional import edge_softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_parquet(\"/home/pervinco/Datasets/leash-bio/preprocessed/molecule_smiles_uniques.parquet\", engine=\"pyarrow\")\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask를 사용하여 Parquet 파일 로드\n",
    "data = dd.read_parquet('/home/pervinco/Datasets/leash-bio/train.parquet')\n",
    "\n",
    "# Unique smiles 추출\n",
    "unique_smiles = data['molecule_smiles'].unique().compute()\n",
    "unique_smiles = unique_smiles.tolist()\n",
    "\n",
    "# Train, validation, test splits\n",
    "train_smiles, temp_smiles = train_test_split(unique_smiles, train_size=0.98, random_state=42)\n",
    "valid_smiles, test_smiles = train_test_split(temp_smiles, test_size=0.64, random_state=42)\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_data = data[data['molecule_smiles'].isin(train_smiles)].compute()\n",
    "valid_data = data[data['molecule_smiles'].isin(valid_smiles)].compute()\n",
    "test_data = data[data['molecule_smiles'].isin(test_smiles)].compute()\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(valid_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1\n",
    "limit = 100000\n",
    "\n",
    "parquet_path = \"/home/pervinco/Datasets/leash-bio/train.parquet\"\n",
    "data_save_path = \"/home/pervinco/Datasets/leash-bio/GNN\"\n",
    "unique_atoms_path = \"./unique_atoms.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.데이터셋을 구성하는 고유한 원자 리스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'c1ccccc1C(=O)O'\n",
    "molecule = Chem.MolFromSmiles(sample)\n",
    "atom_list = molecule.GetAtoms()\n",
    "print(atom_list)\n",
    "\n",
    "for atom in atom_list:\n",
    "    print(atom.GetSymbol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_atoms(smiles_list):\n",
    "    unique_atoms = set()\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "        if molecule:\n",
    "            for atom in molecule.GetAtoms():\n",
    "                unique_atoms.add(atom.GetSymbol())\n",
    "    \n",
    "    return unique_atoms\n",
    "\n",
    "def generate_datasets(parquet_path, n_iter, limit, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    offset_0 = 0\n",
    "    offset_1 = 0\n",
    "    all_unique_atoms = set()\n",
    "\n",
    "    try:\n",
    "        con = duckdb.connect()\n",
    "        for i in range(n_iter):\n",
    "            data_0 = None\n",
    "            data_1 = None\n",
    "            try:\n",
    "                data_0 = con.query(f\"\"\"\n",
    "                    SELECT id, molecule_smiles, protein_name, binds\n",
    "                    FROM parquet_scan('{parquet_path}')\n",
    "                    WHERE binds = 0\n",
    "                    ORDER BY random()\n",
    "                    LIMIT {limit} OFFSET {offset_0}\n",
    "                \"\"\").df()\n",
    "\n",
    "                data_1 = con.query(f\"\"\"\n",
    "                    SELECT id, molecule_smiles, protein_name, binds\n",
    "                    FROM parquet_scan('{parquet_path}')\n",
    "                    WHERE binds = 1\n",
    "                    ORDER BY random()\n",
    "                    LIMIT {limit} OFFSET {offset_1}\n",
    "                \"\"\").df()\n",
    "            except Exception as e:\n",
    "                print(f\"Query failed: {e}\")\n",
    "                break\n",
    "\n",
    "            if data_1.empty:\n",
    "                try:\n",
    "                    data_1 = con.query(f\"\"\"\n",
    "                        SELECT id, molecule_smiles, protein_name, binds\n",
    "                        FROM parquet_scan('{parquet_path}')\n",
    "                        WHERE binds = 1\n",
    "                        ORDER BY random()\n",
    "                        LIMIT {limit}\n",
    "                    \"\"\").df()\n",
    "                except Exception as e:\n",
    "                    print(f\"Query failed: {e}\")\n",
    "                    break\n",
    "\n",
    "            data = pd.concat([data_0, data_1])\n",
    "            smiles_list = data['molecule_smiles'].tolist()\n",
    "            unique_atoms = extract_unique_atoms(smiles_list)\n",
    "            all_unique_atoms.update(unique_atoms)\n",
    "\n",
    "            data = data.sample(frac=1).reset_index(drop=True)\n",
    "            binds_0_count = data[data['binds'] == 0].shape[0]\n",
    "            binds_1_count = data[data['binds'] == 1].shape[0]\n",
    "            print(f\"Iter {i+1} : Dataset shape : {data.shape}, binds=0 count : {binds_0_count}, binds=1 count : {binds_1_count}\")\n",
    "\n",
    "            offset_0 += limit\n",
    "            offset_1 += limit\n",
    "            \n",
    "            table = pq.Table.from_pandas(data)\n",
    "            pq.write_table(table, f\"{save_path}/dataset-{i:>04}.parquet\")\n",
    "            del data_0, data_1, data, table\n",
    "            gc.collect()\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    with open(f\"{save_path}/unique_atoms.txt\", 'w') as file:\n",
    "        for atom in all_unique_atoms:\n",
    "            file.write(f\"{atom}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chunk만큼의 데이터 묶음을 하나의 Dataset이라 가정.\n",
    "class LeasBioDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pq.read_pandas(data_path).to_pandas()\n",
    "        self.train_smiles = list(data['molecule_smiles'])\n",
    "        self.train_labels = list(data['binds'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_smiles[idx], self.train_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeasBioDataset(\"/home/pervinco/Datasets/leash-bio/gnn/dataset-0000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles, labels = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOM_VOCAB = [] ## TODO : 고유한 원자 리스트로 만들기.\n",
    "\n",
    "def one_of_k_encoding(x, vocab):\n",
    "\tif x not in vocab:\n",
    "\t\tx = vocab[-1]\n",
    "            \n",
    "\treturn list(map(lambda s: float(x==s), vocab))\n",
    "\n",
    "\n",
    "def get_atom_feature(atom):\n",
    "\tatom_feature = one_of_k_encoding(atom.GetSymbol(), ATOM_VOCAB)\n",
    "\tatom_feature += one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n",
    "\tatom_feature += one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4])\n",
    "\tatom_feature += one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n",
    "\tatom_feature += [atom.GetIsAromatic()]\n",
    "\treturn atom_feature\n",
    "\n",
    "\n",
    "def get_bond_feature(bond):\n",
    "\tbt = bond.GetBondType()\n",
    "\tbond_feature = [\n",
    "\t\tbt == Chem.rdchem.BondType.SINGLE,\n",
    "\t\tbt == Chem.rdchem.BondType.DOUBLE,\n",
    "\t\tbt == Chem.rdchem.BondType.TRIPLE,\n",
    "\t\tbt == Chem.rdchem.BondType.AROMATIC,\n",
    "\t\tbond.GetIsConjugated(),\n",
    "\t\tbond.IsInRing()\n",
    "\t]\n",
    "\treturn bond_feature\n",
    "\n",
    "\n",
    "def get_molecular_graph(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    graph = dgl.DGLGraph()\n",
    "\n",
    "    ## 원자 수(노드 수)\n",
    "    atom_list = molecule.GetAtoms()\n",
    "    num_atoms = len(atom_list)\n",
    "    graph.add_nodes(num_atoms)\n",
    "\n",
    "    ## 원자의 특징(Atom Features)\n",
    "    # 원자 특징들을 계산 -> 정수형 특징값들 -> 배열\n",
    "    atom_feature_list = torch.tensor([get_atom_feature(atom) for atom in atom_list], dtype=torch.float64)\n",
    "    graph.ndata['h'] = atom_feature_list\n",
    "\n",
    "    ## 연결의 특징(Edge Features)\n",
    "    bond_feature_list = []\n",
    "    bond_list = molecule.GetBonds() ## 분자가 가진 bond들을 구함.\n",
    "    for bond in bond_list:\n",
    "          ## 각각의 bond가 가진 특징값들을 계산.\n",
    "          # 연결 특징들을 계산 -> 정수형 특징값들 -> 배열\n",
    "          bond_feature = get_bond_feature(bond)\n",
    "\n",
    "          src = bond.GetBeginAtom().GetIdx() ## 결합의 시작 원자의 인덱스\n",
    "          dst = bond.GetEndAtom().GetIdx() ## 결합의  원자의 인덱스\n",
    "\n",
    "          ## 그래프에 시작 원자에서 끝 원자로의 방향성을 가진 엣지를 추가\n",
    "          graph.add_edges(src, dst)\n",
    "          bond_feature_list.append(bond_feature)\n",
    "\n",
    "          ## DGL 그래프는 비방향성 그래프. 따라서 반대 방향의 엣지를 추가\n",
    "          graph.add_edges(dst, src)\n",
    "          bond_feature_list.append(bond_feature)\n",
    "\n",
    "    bond_feature_list = torch.tensor(bond_feature_list, dtype=torch.float64)\n",
    "    graph.edata['e_ij'] = bond_feature_list\n",
    "    return graph\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph_list, label_list = [], []\n",
    "    for item in batch:\n",
    "        smiles, label = item[0], item[1]\n",
    "        graph = get_molecular_graph(smiles)\n",
    "\n",
    "        graph_list.append(graph)\n",
    "        label_list.append(label)\n",
    "\n",
    "    graph_list = dgl.batch(graph_list)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float64)\n",
    "\n",
    "    return graph_list, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bias=False, activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GraphAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4, mlp_bias=False, drop_prob=0.2, activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = hidden_dim // num_heads\n",
    "        self.prob = drop_prob\n",
    "        self.activation = activation\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(input_dim=hidden_dim, hidden_dim=2*hidden_dim, output_dim=hidden_dim, bias=mlp_bias, activation=activation)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.w1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w4 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w5 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w6 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, graph:dgl.DGLGraph):\n",
    "        h0 = graph.ndata['h'] ## graph nodes\n",
    "        e_ij = graph.edata['e_ij'] ## graph edges\n",
    "\n",
    "        graph.ndata['u'] = self.w1(h0).view(-1, self.num_heads, self.dk)\n",
    "        graph.ndata['v'] = self.w2(h0).view(-1, self.num_heads, self.dk)\n",
    "        graph.edata['x_ij'] = self.w3(e_ij).view(-1, self.num_heads, self.dk)\n",
    "\n",
    "        graph.apply_edges(func.v_add_e('v', 'x_ij', 'm'))\n",
    "        graph.apply_edges(func.u_mul_e('u', 'm', 'attn'))\n",
    "        graph.edata['attn'] = edge_softmax(graph, graph.edata['attn'] / sqrt(self.dk))\n",
    "\n",
    "        graph.ndata['k'] = self.w4(h0).view(-1, self.num_heads, self.dk)\n",
    "        graph.edata['x_ij'] = self.w5(e_ij).view(-1, self.num_heads, self.dk)\n",
    "        graph.apply_edges(func.v_add_e('k', 'x_ij', 'm'))\n",
    "\n",
    "        graph.edata['m'] = graph.edata['attn'] * graph.edata['m']\n",
    "        graph.update_all(func.copy_edge('m', 'm'), func.sum('m', 'h'))\n",
    "\n",
    "        h = self.w6(h0) + graph.ndata['h'].view(-1, self.hidden_dim)\n",
    "        h = self.norm(h)\n",
    "\n",
    "        h = h + self.mlp(h)\n",
    "        h = self.norm(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        graph.ndata['h'] = h \n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Modile):\n",
    "    def __init__(self, \n",
    "                 num_layers=5, \n",
    "                 hidden_dim=64, \n",
    "                 num_heads=4, \n",
    "                 drop_prob=0.2, \n",
    "                 mlp_bias=False, \n",
    "                 readout='sum', \n",
    "                 activation=F.relu, \n",
    "                 initial_node_dim=58, \n",
    "                 initial_edge_dim=6):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.readout = readout\n",
    "\n",
    "        self.node_embedding = nn.Linear(initial_node_dim, hidden_dim, bias=False)\n",
    "        self.edge_embedding = nn.Linear(initial_edge_dim, hidden_dim, bias=False)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = GraphAttention(hidden_dim, num_heads, mlp_bias, drop_prob, activation)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.output = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.sigmoid = F.sigmoid\n",
    "\n",
    "    def forward(self, graph:dgl.DGLGraph):\n",
    "        h = self.node_embedding(graph.ndata['h'].float())\n",
    "        e_ij = self.edge_embedding(graph.edata['e_ij'].float())\n",
    "\n",
    "        graph.ndata['h'] = h\n",
    "        graph.edata['e_ij'] = e_ij\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            graph = self.layers[i](graph)\n",
    "\n",
    "        out = dgl.readout_nodes(graph, 'h', op=self.readout)\n",
    "        out = self.output(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"PyTorch Geometric version:\", torch_geometric.__version__)\n",
    "\n",
    "# 테스트용 데이터 생성\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# 간단한 GCN 레이어 생성 및 적용\n",
    "conv = GCNConv(1, 2)\n",
    "x = conv(data.x, data.edge_index)\n",
    "\n",
    "print(\"GCN layer output:\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
