{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import fastparquet\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pervinco/Datasets/leash-bio\"\n",
    "input_file = f'{data_dir}/train.parquet'\n",
    "output_files = {\n",
    "    'train': f'{data_dir}/train_split.parquet',\n",
    "    'valid': f'{data_dir}/valid_split.parquet',\n",
    "    'test': f'{data_dir}/test_split.parquet'\n",
    "}\n",
    "\n",
    "num_proteins = 3\n",
    "train_size_per_protein = 98000000\n",
    "valid_size_per_protein = 200000\n",
    "test_size_per_protein = 360000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "train_writer = None\n",
    "valid_writer = None\n",
    "test_writer = None\n",
    "\n",
    "# Parquet 파일에서 데이터 row group 단위로 읽기\n",
    "parquet_file = pq.ParquetFile(input_file)\n",
    "\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    # row group 단위로 데이터 읽기\n",
    "    chunk = parquet_file.read_row_groups([i]).to_pandas()\n",
    "\n",
    "    X = chunk.drop(columns=['binds'])  # 특성 데이터\n",
    "    y = chunk['binds']  # 라벨 데이터\n",
    "\n",
    "    # Train과 나머지(valid+test) 분할\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Validation과 Test 분할\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # 데이터 합치기\n",
    "    train_chunk = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_chunk = pd.concat([X_valid, y_valid], axis=1)\n",
    "    test_chunk = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # ParquetWriter 초기화 또는 데이터 추가\n",
    "    if train_writer is None:\n",
    "        train_writer = pq.ParquetWriter(f'{data_dir}/train_split.parquet', pa.Table.from_pandas(train_chunk).schema)\n",
    "        valid_writer = pq.ParquetWriter(f'{data_dir}/valid_split.parquet', pa.Table.from_pandas(valid_chunk).schema)\n",
    "        test_writer = pq.ParquetWriter(f'{data_dir}/test_split.parquet', pa.Table.from_pandas(test_chunk).schema)\n",
    "    else:\n",
    "        train_writer.write_table(pa.Table.from_pandas(train_chunk))\n",
    "        valid_writer.write_table(pa.Table.from_pandas(valid_chunk))\n",
    "        test_writer.write_table(pa.Table.from_pandas(test_chunk))\n",
    "\n",
    "# ParquetWriter 닫기\n",
    "if train_writer:\n",
    "    train_writer.close()\n",
    "    valid_writer.close()\n",
    "    test_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binds=0: 234087381.0, binds=1: 1270998.0\n",
      "binds=0: 29261734.0, binds=1: 158203.0\n",
      "binds=0: 29261916.0, binds=1: 158022.0\n",
      "Train Set Class Distribution:\n",
      "binds\n",
      "0    99.459973\n",
      "1     0.540027\n",
      "dtype: float64\n",
      "\n",
      "Validation Set Class Distribution:\n",
      "binds\n",
      "0    99.462259\n",
      "1     0.537741\n",
      "dtype: float64\n",
      "\n",
      "Test Set Class Distribution:\n",
      "binds\n",
      "0    99.462874\n",
      "1     0.537126\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def check_class_distribution(file_path, chunk_size=1000000):\n",
    "    # Parquet 파일 읽기 초기화\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    class_counts = pd.Series(dtype=int)\n",
    "\n",
    "    # 청크 단위로 데이터 읽기\n",
    "    for i in range(num_row_groups):\n",
    "        chunk = parquet_file.read_row_groups([i]).to_pandas()\n",
    "        class_counts = class_counts.add(chunk['binds'].value_counts(), fill_value=0)\n",
    "\n",
    "    # 클래스 분포 계산\n",
    "    total = class_counts.sum()\n",
    "    distribution = (class_counts / total) * 100\n",
    "\n",
    "    # binds=0과 binds=1의 갯수 출력\n",
    "    binds_0_count = class_counts.get(0, 0)\n",
    "    binds_1_count = class_counts.get(1, 0)\n",
    "    print(f\"binds=0: {binds_0_count}, binds=1: {binds_1_count}\")\n",
    "\n",
    "    return distribution\n",
    "\n",
    "train_file = f'{data_dir}/train_split.parquet'\n",
    "valid_file = f'{data_dir}/valid_split.parquet'\n",
    "test_file = f'{data_dir}/test_split.parquet'\n",
    "\n",
    "train_distribution = check_class_distribution(train_file)\n",
    "valid_distribution = check_class_distribution(valid_file)\n",
    "test_distribution = check_class_distribution(test_file)\n",
    "\n",
    "print(\"Train Set Class Distribution:\")\n",
    "print(train_distribution)\n",
    "print(\"\\nValidation Set Class Distribution:\")\n",
    "print(valid_distribution)\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(test_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unique_molecules(file_path):\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    unique_molecules = set()\n",
    "\n",
    "    for i in range(parquet_file.num_row_groups):\n",
    "        chunk = parquet_file.read_row_groups([i], columns=['molecule_smiles']).to_pandas()\n",
    "        unique_molecules.update(chunk['molecule_smiles'])\n",
    "\n",
    "    return unique_molecules\n",
    "\n",
    "unique_molecules_file = f\"{data_dir}/preprocessed/molecule_smiles_uniques.parquet\"\n",
    "unique_molecules = read_unique_molecules(unique_molecules_file)\n",
    "print(f\"Total unique molecules: {len(unique_molecules)}\")\n",
    "print(unique_molecules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "def check_unique_molecules(file_path, unique_molecules):\n",
    "    # Parquet 파일 읽기 초기화\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    molecules_in_file = set()\n",
    "\n",
    "    # 청크 단위로 데이터 읽기\n",
    "    for i in range(num_row_groups):\n",
    "        chunk = parquet_file.read_row_groups([i]).to_pandas()\n",
    "        molecules_in_file.update(chunk['molecule_smiles'])\n",
    "\n",
    "    # 고유 molecule 값 포함 여부 확인\n",
    "    missing_molecules = unique_molecules - molecules_in_file\n",
    "    return missing_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = f'{data_dir}/train_split.parquet'\n",
    "missing_in_train = check_unique_molecules(train_file, unique_molecules)\n",
    "\n",
    "if not missing_in_train:\n",
    "    print(\"All unique molecules are present in the train set.\")\n",
    "else:\n",
    "    print(f\"Missing molecules in the train set: {missing_in_train}\")\n",
    "\n",
    "del missing_in_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = f'{data_dir}/valid_split.parquet'\n",
    "missing_in_valid = check_unique_molecules(valid_file, unique_molecules)\n",
    "\n",
    "if not missing_in_valid:\n",
    "    print(\"All unique molecules are present in the valid set.\")\n",
    "else:\n",
    "    print(f\"Missing molecules in the valid set: {missing_in_valid}\")\n",
    "\n",
    "del missing_in_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f'{data_dir}/test_split.parquet'\n",
    "missing_in_test = check_unique_molecules(test_file, unique_molecules)\n",
    "\n",
    "if not missing_in_test:\n",
    "    print(\"All unique molecules are present in the test set.\")\n",
    "else:\n",
    "    print(f\"Missing molecules in the test set: {missing_in_test}\")\n",
    "\n",
    "del missing_in_train\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
